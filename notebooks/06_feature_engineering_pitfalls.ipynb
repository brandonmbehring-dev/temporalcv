{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Feature Engineering Pitfalls for Time Series\n\n## Common Mistakes That Create Lookahead Bias\n\n---\n\n## ðŸš¨ If You Know sklearn But Not Time Series Feature Engineering, Read This First\n\n**What you already know (from standard ML)**:\n- `StandardScaler().fit(X)` on the whole dataset is fine\n- `df['feature'] = df['value'].rolling(7).mean()` before split is fine\n- `SelectKBest().fit(X, y)` on all data is fine\n- Centered windows smooth nicely\n\n**What's different with time series** (every one of these is a bug!):\n\n| Standard ML | Time Series Bug | Why It's Wrong |\n|-------------|-----------------|----------------|\n| Fit scaler on all X | Train info leaks to test | Scaler knows test distribution |\n| Rolling mean before split | Future leaks to past | rolling_mean[t=100] uses t=101,102... |\n| SelectKBest on all data | Feature selection leaks | Selection optimized for test |\n| Centered window | Uses future values | `center=True` looks ahead |\n\n**The pattern**: Anything computed \"before split\" or \"on all data\" leaks future information.\n\n**Real-world impact**: A practitioner computed a 7-day rolling mean on the full series before splitting. The model reported 35% improvement over baseline. After fixing: 3% improvement. The 32% was pure leakage.\n\n---\n\n**What you'll learn:**\n1. Why standard feature engineering patterns create leakage in time series\n2. How rolling statistics, feature selection, and windowed calculations can look ahead\n3. Safe patterns for creating lag features and rolling stats\n4. How to validate your features with `gate_shuffled_target`\n\n**Prerequisites:** Notebooks 01, 05\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from temporalcv.cv import WalkForwardCV\n",
    "from temporalcv.gates import gate_shuffled_target, GateStatus\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate AR(1) process\n",
    "def generate_ar1(n=500, phi=0.9, sigma=1.0, seed=42):\n",
    "    \"\"\"Generate AR(1) process.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.zeros(n)\n",
    "    y[0] = rng.normal(0, sigma / np.sqrt(1 - phi**2))\n",
    "    for t in range(1, n):\n",
    "        y[t] = phi * y[t-1] + sigma * rng.normal()\n",
    "    return y\n",
    "\n",
    "# Generate data\n",
    "series = generate_ar1(n=600, phi=0.9, seed=42)\n",
    "print(f\"Generated series with {len(series)} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Bug Category #2 â€” Rolling Statistics Before Split\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Computing rolling statistics on the **full series** before train/test split uses future information.\n",
    "\n",
    "```python\n",
    "# WRONG: Rolling mean computed on full series\n",
    "df['rolling_mean'] = df['target'].rolling(10).mean()\n",
    "train, test = df[:split], df[split:]\n",
    "# Bug: Test features used future data!\n",
    "```\n",
    "\n",
    "### Why It's Wrong\n",
    "\n",
    "The rolling mean at time `t` in the test set includes information from times `t-9` to `t`. If `t-9` is in the test period, you've leaked test data into training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the rolling stats bug\n",
    "window = 10\n",
    "split_idx = 400\n",
    "\n",
    "# WRONG: Rolling mean on full series\n",
    "rolling_mean_wrong = pd.Series(series).rolling(window).mean().values\n",
    "\n",
    "# Show where the problem occurs\n",
    "print(\"Rolling Statistics Bug\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWindow size: {window}\")\n",
    "print(f\"Split index: {split_idx}\")\n",
    "print(f\"\\nFor test observation at index {split_idx}:\")\n",
    "print(f\"  Rolling mean uses indices: {split_idx - window + 1} to {split_idx}\")\n",
    "print(f\"  This is FINE (all from training period)\")\n",
    "\n",
    "print(f\"\\nFor test observation at index {split_idx + 5}:\")\n",
    "print(f\"  Rolling mean uses indices: {split_idx + 5 - window + 1} to {split_idx + 5}\")\n",
    "print(f\"  Some of these ({split_idx}, {split_idx + 1}, ...) are from TEST period!\")\n",
    "print(f\"  BUG: This rolling mean 'knows' test period values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the problem\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "t = np.arange(len(series))\n",
    "\n",
    "# Mark train/test regions\n",
    "ax.axvspan(0, split_idx, alpha=0.2, color='blue', label='Train period')\n",
    "ax.axvspan(split_idx, len(series), alpha=0.2, color='red', label='Test period')\n",
    "\n",
    "# Plot series and rolling mean\n",
    "ax.plot(t, series, 'b-', alpha=0.5, linewidth=0.5, label='Original')\n",
    "ax.plot(t, rolling_mean_wrong, 'r-', linewidth=2, label=f'Rolling mean (window={window})')\n",
    "\n",
    "# Highlight problematic region\n",
    "ax.axvspan(split_idx, split_idx + window, alpha=0.5, color='orange', \n",
    "           label='Contaminated region')\n",
    "\n",
    "ax.set_xlabel('Time Index', fontsize=11)\n",
    "ax.set_ylabel('Value', fontsize=11)\n",
    "ax.set_title('Rolling Statistics Bug: Test Features Use Future Data',\n",
    "             fontsize=13, fontweight='bold', color='red')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(split_idx - 50, split_idx + 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe orange region shows where rolling mean is 'contaminated' by test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fix: Compute rolling stats within each fold\n",
    "def create_features_wrong(series):\n",
    "    \"\"\"WRONG: Rolling stats on full series.\"\"\"\n",
    "    df = pd.DataFrame({'y': series})\n",
    "    df['lag_1'] = df['y'].shift(1)\n",
    "    df['rolling_mean'] = df['y'].rolling(10).mean()  # BUG: Uses future!\n",
    "    df['rolling_std'] = df['y'].rolling(10).std()    # BUG: Uses future!\n",
    "    return df.dropna()\n",
    "\n",
    "def create_features_right(series, is_train_mask):\n",
    "    \"\"\"RIGHT: Rolling stats computed separately for train.\"\"\"\n",
    "    df = pd.DataFrame({'y': series})\n",
    "    df['lag_1'] = df['y'].shift(1)\n",
    "    \n",
    "    # Compute rolling stats ONLY on training data\n",
    "    train_series = df['y'].copy()\n",
    "    train_series[~is_train_mask] = np.nan  # Mask test data\n",
    "    \n",
    "    df['rolling_mean'] = train_series.rolling(10, min_periods=1).mean()\n",
    "    df['rolling_std'] = train_series.rolling(10, min_periods=1).std()\n",
    "    \n",
    "    # Forward fill for test period (use last known training value)\n",
    "    df['rolling_mean'] = df['rolling_mean'].ffill()\n",
    "    df['rolling_std'] = df['rolling_std'].ffill()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "# Compare\n",
    "print(\"Comparing WRONG vs RIGHT Approaches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_wrong = create_features_wrong(series)\n",
    "print(f\"\\nWRONG approach: Rolling stats computed on full series\")\n",
    "print(f\"  At index {split_idx + 5}:\")\n",
    "print(f\"  rolling_mean = {df_wrong.loc[split_idx + 5, 'rolling_mean']:.4f}\")\n",
    "\n",
    "is_train = np.zeros(len(series), dtype=bool)\n",
    "is_train[:split_idx] = True\n",
    "df_right = create_features_right(series, is_train)\n",
    "print(f\"\\nRIGHT approach: Rolling stats from training only\")\n",
    "print(f\"  At index {split_idx + 5}:\")\n",
    "print(f\"  rolling_mean = {df_right.loc[split_idx + 5, 'rolling_mean']:.4f}\")\n",
    "print(f\"  (Uses last known training mean, forward-filled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Bug Category #4 â€” Feature Selection on Full Data\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Selecting features using the **full dataset** (including test data) creates leakage.\n",
    "\n",
    "```python\n",
    "# WRONG: Select features on full data\n",
    "selector = SelectKBest(k=5).fit(X_all, y_all)\n",
    "X_selected = selector.transform(X_all)\n",
    "train, test = X_selected[:split], X_selected[split:]\n",
    "# Bug: Selection used test data information!\n",
    "```\n",
    "\n",
    "### Why It's Wrong\n",
    "\n",
    "Feature selection scores (correlation, F-statistics) computed on the full dataset include test-period target values. The selected features are \"optimized\" for test data they shouldn't know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate feature selection leakage\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Create features: some useful, some noise\n",
    "n = len(series)\n",
    "n_features = 20\n",
    "n_useful = 5\n",
    "\n",
    "# First n_useful features have some signal\n",
    "X_all = np.zeros((n, n_features))\n",
    "for i in range(n_useful):\n",
    "    X_all[:, i] = np.roll(series, i + 1)  # Lag features\n",
    "X_all[:(n_useful), :n_useful] = np.nan  # Clean up NaNs\n",
    "\n",
    "# Rest are noise\n",
    "X_all[:, n_useful:] = rng.normal(0, 1, (n, n_features - n_useful))\n",
    "\n",
    "# Remove NaN rows\n",
    "valid = ~np.isnan(X_all).any(axis=1)\n",
    "X_all = X_all[valid]\n",
    "y_all = series[valid]\n",
    "\n",
    "# Split\n",
    "split = int(len(X_all) * 0.7)\n",
    "X_train, X_test = X_all[:split], X_all[split:]\n",
    "y_train, y_test = y_all[:split], y_all[split:]\n",
    "\n",
    "print(\"Feature Selection Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal features: {n_features}\")\n",
    "print(f\"Useful features: {n_useful} (lag features)\")\n",
    "print(f\"Noise features: {n_features - n_useful}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Feature selection on full data\n",
    "selector_wrong = SelectKBest(score_func=f_regression, k=5)\n",
    "selector_wrong.fit(X_all, y_all)  # Uses ALL data!\n",
    "selected_wrong = selector_wrong.get_support(indices=True)\n",
    "\n",
    "# RIGHT: Feature selection on training data only\n",
    "selector_right = SelectKBest(score_func=f_regression, k=5)\n",
    "selector_right.fit(X_train, y_train)  # Uses only training!\n",
    "selected_right = selector_right.get_support(indices=True)\n",
    "\n",
    "print(\"\\nWRONG: Selection on full data\")\n",
    "print(f\"  Selected features: {selected_wrong}\")\n",
    "\n",
    "print(\"\\nRIGHT: Selection on training only\")\n",
    "print(f\"  Selected features: {selected_right}\")\n",
    "\n",
    "# Check which are actually useful (lag features are indices 0-4)\n",
    "useful_idx = set(range(n_useful))\n",
    "print(f\"\\nActual useful features: {list(useful_idx)}\")\n",
    "print(f\"WRONG correctly identified: {len(set(selected_wrong) & useful_idx)}/{n_useful}\")\n",
    "print(f\"RIGHT correctly identified: {len(set(selected_right) & useful_idx)}/{n_useful}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for leakage with gate_shuffled_target\n",
    "print(\"Leakage Detection with gate_shuffled_target\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# Test WRONG approach\n",
    "X_selected_wrong = selector_wrong.transform(X_all)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result_wrong = gate_shuffled_target(\n",
    "        model, X_selected_wrong, y_all,\n",
    "        method='effect_size',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nWRONG (selection on full data):\")\n",
    "print(f\"  Status: {result_wrong.status.value}\")\n",
    "print(f\"  Improvement: {result_wrong.details.get('improvement_ratio', 0)*100:.1f}%\")\n",
    "\n",
    "# Test RIGHT approach (selection within CV)\n",
    "# For proper testing, we need to do selection inside CV\n",
    "print(f\"\\nRIGHT approach requires selection INSIDE each CV fold.\")\n",
    "print(f\"See the safe feature engineering template below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Bug Category #6 â€” Centered Windows\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Centered window calculations include **future** observations.\n",
    "\n",
    "```python\n",
    "# WRONG: Centered rolling mean\n",
    "smoothed[t] = np.mean(series[t-3:t+4])  # Uses t+1, t+2, t+3!\n",
    "\n",
    "# RIGHT: Backward-only window\n",
    "smoothed[t] = np.mean(series[t-6:t+1])  # Uses only past + present\n",
    "```\n",
    "\n",
    "### Common Culprits\n",
    "- `center=True` in pandas rolling\n",
    "- Symmetric smoothing filters\n",
    "- Moving average with equal weights on both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate centered vs backward-only windows\n",
    "window = 7\n",
    "\n",
    "# Centered window (WRONG for time series)\n",
    "centered = pd.Series(series).rolling(window, center=True).mean().values\n",
    "\n",
    "# Backward-only window (RIGHT)\n",
    "backward = pd.Series(series).rolling(window, center=False).mean().values\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "t_demo = 50  # Time point to demonstrate\n",
    "t_range = np.arange(t_demo - 10, t_demo + 10)\n",
    "\n",
    "# Left: Centered (WRONG)\n",
    "ax = axes[0]\n",
    "ax.plot(t_range, series[t_range], 'b-o', markersize=4, label='Series')\n",
    "ax.axvline(x=t_demo, color='black', linestyle='--', label=f'Time t={t_demo}')\n",
    "\n",
    "# Highlight window for centered\n",
    "half = window // 2\n",
    "ax.axvspan(t_demo - half, t_demo + half, alpha=0.3, color='red', \n",
    "           label=f'Centered window')\n",
    "ax.scatter([t_demo], [centered[t_demo]], color='red', s=100, zorder=5,\n",
    "           marker='X', label=f'Centered mean')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title(f'WRONG: Centered Window (center=True)\\nUses FUTURE data!',\n",
    "             fontsize=12, fontweight='bold', color='red')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "# Right: Backward (RIGHT)\n",
    "ax = axes[1]\n",
    "ax.plot(t_range, series[t_range], 'b-o', markersize=4, label='Series')\n",
    "ax.axvline(x=t_demo, color='black', linestyle='--', label=f'Time t={t_demo}')\n",
    "\n",
    "# Highlight window for backward\n",
    "ax.axvspan(t_demo - window + 1, t_demo + 1, alpha=0.3, color='green',\n",
    "           label=f'Backward window')\n",
    "ax.scatter([t_demo], [backward[t_demo]], color='green', s=100, zorder=5,\n",
    "           marker='o', label=f'Backward mean')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title(f'RIGHT: Backward Window (center=False)\\nUses only PAST data',\n",
    "             fontsize=12, fontweight='bold', color='green')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAt time t={t_demo}:\")\n",
    "print(f\"  Centered mean (WRONG): uses data from t-{half} to t+{half}\")\n",
    "print(f\"  Backward mean (RIGHT): uses data from t-{window-1} to t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test centered vs backward windows for leakage\n",
    "print(\"Centered vs Backward Window Leakage Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create features with centered window (WRONG)\n",
    "df = pd.DataFrame({'y': series})\n",
    "df['lag_1'] = df['y'].shift(1)\n",
    "df['centered_mean'] = df['y'].rolling(7, center=True).mean()  # WRONG!\n",
    "df_wrong = df.dropna()\n",
    "\n",
    "X_centered = df_wrong[['lag_1', 'centered_mean']].values\n",
    "y_centered = df_wrong['y'].values\n",
    "\n",
    "# Create features with backward window (RIGHT)\n",
    "df = pd.DataFrame({'y': series})\n",
    "df['lag_1'] = df['y'].shift(1)\n",
    "df['backward_mean'] = df['y'].rolling(7, center=False).mean()  # RIGHT\n",
    "df_right = df.dropna()\n",
    "\n",
    "X_backward = df_right[['lag_1', 'backward_mean']].values\n",
    "y_backward = df_right['y'].values\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# Test centered\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result_centered = gate_shuffled_target(\n",
    "        model, X_centered, y_centered,\n",
    "        method='permutation',\n",
    "        n_shuffles=30,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nCentered window (center=True):\")\n",
    "print(f\"  Status: {result_centered.status.value}\")\n",
    "print(f\"  p-value: {result_centered.details.get('pvalue', 'N/A'):.4f}\")\n",
    "\n",
    "# Test backward\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result_backward = gate_shuffled_target(\n",
    "        model, X_backward, y_backward,\n",
    "        method='permutation',\n",
    "        n_shuffles=30,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nBackward window (center=False):\")\n",
    "print(f\"  Status: {result_backward.status.value}\")\n",
    "print(f\"  p-value: {result_backward.details.get('pvalue', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Safe Feature Engineering Template\n",
    "\n",
    "### The Golden Rule\n",
    "\n",
    "**All statistics must be computed using only past data relative to each observation.**\n",
    "\n",
    "### Safe Pattern: Compute Inside CV Folds\n",
    "\n",
    "```python\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    # Compute features using ONLY training data\n",
    "    train_mean = y[train_idx].mean()\n",
    "    train_std = y[train_idx].std()\n",
    "    \n",
    "    # Apply to both train and test\n",
    "    X_train_normalized = (X[train_idx] - train_mean) / train_std\n",
    "    X_test_normalized = (X[test_idx] - train_mean) / train_std  # Same stats!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe feature engineering template\n",
    "class SafeFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering that respects temporal boundaries.\n",
    "    \n",
    "    All statistics computed during fit() use only training data.\n",
    "    Transform() applies the same statistics to new data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window=10):\n",
    "        self.window = window\n",
    "        self.train_mean = None\n",
    "        self.train_std = None\n",
    "        \n",
    "    def fit(self, series, y=None):\n",
    "        \"\"\"Compute statistics from training data only.\"\"\"\n",
    "        self.train_mean = np.mean(series)\n",
    "        self.train_std = np.std(series)\n",
    "        \n",
    "        # Store last window values for rolling features\n",
    "        self.last_window = series[-self.window:].copy()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, series):\n",
    "        \"\"\"Create features using only past information.\"\"\"\n",
    "        n = len(series)\n",
    "        features = []\n",
    "        \n",
    "        # Lag features (always safe)\n",
    "        for lag in [1, 2, 3]:\n",
    "            lagged = np.concatenate([[np.nan]*lag, series[:-lag]])\n",
    "            features.append(lagged)\n",
    "        \n",
    "        # Backward-only rolling mean\n",
    "        rolling = pd.Series(series).rolling(self.window, min_periods=1).mean().values\n",
    "        features.append(rolling)\n",
    "        \n",
    "        # Normalized using training statistics\n",
    "        normalized = (series - self.train_mean) / self.train_std\n",
    "        features.append(normalized)\n",
    "        \n",
    "        X = np.column_stack(features)\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, series):\n",
    "        return self.fit(series).transform(series)\n",
    "\n",
    "\n",
    "# Demonstrate safe feature engineering in CV\n",
    "print(\"Safe Feature Engineering in Walk-Forward CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cv = WalkForwardCV(n_splits=5, window_type='expanding', test_size=50)\n",
    "model = Ridge(alpha=1.0)\n",
    "engineer = SafeFeatureEngineer(window=10)\n",
    "\n",
    "all_maes = []\n",
    "for i, (train_idx, test_idx) in enumerate(cv.split(series)):\n",
    "    # Fit engineer on TRAINING data only\n",
    "    engineer.fit(series[train_idx])\n",
    "    \n",
    "    # Transform both sets using training statistics\n",
    "    X_train = engineer.transform(series[train_idx])\n",
    "    X_test = engineer.transform(series[test_idx])\n",
    "    y_train = series[train_idx]\n",
    "    y_test = series[test_idx]\n",
    "    \n",
    "    # Remove NaN rows\n",
    "    valid_train = ~np.isnan(X_train).any(axis=1)\n",
    "    valid_test = ~np.isnan(X_test).any(axis=1)\n",
    "    \n",
    "    X_train = X_train[valid_train]\n",
    "    y_train = y_train[valid_train]\n",
    "    X_test = X_test[valid_test]\n",
    "    y_test = y_test[valid_test]\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    all_maes.append(mae)\n",
    "    \n",
    "    print(f\"  Fold {i+1}: Train size={len(X_train)}, Test size={len(X_test)}, MAE={mae:.4f}\")\n",
    "\n",
    "print(f\"\\nMean MAE: {np.mean(all_maes):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the safe approach has no leakage\n",
    "print(\"\\nValidating Safe Approach with gate_shuffled_target\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create features for full validation\n",
    "engineer_full = SafeFeatureEngineer(window=10)\n",
    "X_safe = engineer_full.fit_transform(series)\n",
    "y_safe = series\n",
    "\n",
    "# Remove NaN rows\n",
    "valid = ~np.isnan(X_safe).any(axis=1)\n",
    "X_safe = X_safe[valid]\n",
    "y_safe = y_safe[valid]\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result = gate_shuffled_target(\n",
    "        model, X_safe, y_safe,\n",
    "        method='permutation',\n",
    "        n_shuffles=50,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nSafe feature engineering:\")\n",
    "print(f\"  Status: {result.status.value}\")\n",
    "print(f\"  p-value: {result.details.get('pvalue', 'N/A'):.4f}\")\n",
    "print(f\"\\n  The safe approach passes the shuffled target test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pitfall Section\n",
    "\n",
    "### Pitfall 1: Rolling Stats Before Split\n",
    "\n",
    "```python\n",
    "# WRONG: Compute on full data before split\n",
    "df['rolling_mean'] = df['y'].rolling(10).mean()\n",
    "train, test = df[:split], df[split:]\n",
    "\n",
    "# RIGHT: Use expanding window or compute in CV\n",
    "# Option A: Expanding window (always looks back)\n",
    "df['expanding_mean'] = df['y'].expanding().mean()\n",
    "\n",
    "# Option B: Compute inside CV folds\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    train_mean = y[train_idx].rolling(10).mean()\n",
    "    # Apply last training mean to test\n",
    "```\n",
    "\n",
    "### Pitfall 2: Feature Selection on Full Data\n",
    "\n",
    "```python\n",
    "# WRONG: Select features using all data\n",
    "selector = SelectKBest(k=5).fit(X_all, y_all)\n",
    "\n",
    "# RIGHT: Select features inside each CV fold\n",
    "for train_idx, test_idx in cv.split(X, y):\n",
    "    selector = SelectKBest(k=5).fit(X[train_idx], y[train_idx])\n",
    "    X_train = selector.transform(X[train_idx])\n",
    "    X_test = selector.transform(X[test_idx])\n",
    "```\n",
    "\n",
    "### Pitfall 3: Centered Windows\n",
    "\n",
    "```python\n",
    "# WRONG: Centered window uses future\n",
    "df['smoothed'] = df['y'].rolling(7, center=True).mean()\n",
    "\n",
    "# RIGHT: Backward-only window\n",
    "df['smoothed'] = df['y'].rolling(7, center=False).mean()\n",
    "```\n",
    "\n",
    "### Pitfall 4: Standardization on Full Data\n",
    "\n",
    "```python\n",
    "# WRONG: Fit scaler on all data\n",
    "scaler = StandardScaler().fit(X_all)\n",
    "\n",
    "# RIGHT: Fit scaler on training only\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Same scaler!\n",
    "```\n",
    "\n",
    "### Pitfall 5: Target Encoding Without Temporal Awareness\n",
    "\n",
    "```python\n",
    "# WRONG: Category means using all data\n",
    "category_means = df.groupby('category')['target'].mean()\n",
    "\n",
    "# RIGHT: Category means from training only\n",
    "train_means = df.loc[:split].groupby('category')['target'].mean()\n",
    "df['category_encoded'] = df['category'].map(train_means)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference: Safe vs Unsafe patterns\n",
    "print(\"QUICK REFERENCE: Safe vs Unsafe Patterns\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "patterns = [\n",
    "    (\"Rolling stats\", \n",
    "     \"df['y'].rolling(10).mean() before split\",\n",
    "     \"Compute inside each CV fold\"),\n",
    "    (\"Feature selection\",\n",
    "     \"SelectKBest().fit(X_all, y_all)\",\n",
    "     \"SelectKBest().fit(X_train, y_train)\"),\n",
    "    (\"Window type\",\n",
    "     \"rolling(7, center=True)\",\n",
    "     \"rolling(7, center=False)\"),\n",
    "    (\"Standardization\",\n",
    "     \"StandardScaler().fit(X_all)\",\n",
    "     \"StandardScaler().fit(X_train)\"),\n",
    "    (\"Target encoding\",\n",
    "     \"groupby('cat')['y'].mean() on full df\",\n",
    "     \"groupby('cat')['y'].mean() on train only\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Operation':<20} {'WRONG':<35} {'RIGHT'}\")\n",
    "print(\"-\" * 90)\n",
    "for op, wrong, right in patterns:\n",
    "    print(f\"{op:<20} {wrong:<35} {right}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### 1. Order Matters in Time Series\n",
    "Every computation must respect temporal ordering. What works in tabular ML creates leakage in time series.\n",
    "\n",
    "### 2. \"Before Split\" = Leakage\n",
    "Any statistic computed before train/test split potentially uses future information.\n",
    "\n",
    "### 3. Centered Windows Look Ahead\n",
    "Always use `center=False` for rolling statistics in time series.\n",
    "\n",
    "### 4. Feature Selection Must Happen Inside CV\n",
    "Selecting features on the full dataset leaks test information into the selection process.\n",
    "\n",
    "### 5. Validate with `gate_shuffled_target`\n",
    "After feature engineering, run the shuffled target test to verify no leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **07_threshold_leakage.ipynb**: Regime and percentile computation without lookahead\n",
    "- **08_validation_workflow.ipynb**: Complete HALT/WARN/PASS pipeline\n",
    "- **05_shuffled_target_gate.ipynb**: Review if needed\n",
    "\n",
    "---\n",
    "\n",
    "*\"In time series, your features can only see the past. Anything else is cheating.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}