{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Time Series Fundamentals for sklearn Practitioners\n",
    "\n",
    "## The Missing Bridge: Why Everything You Know About ML Validation is Wrong for Time Series\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Who This Notebook is For\n",
    "\n",
    "You should read this notebook if:\n",
    "- \u2705 You're comfortable with sklearn's `KFold`, `train_test_split`, and metrics like MAE/RMSE\n",
    "- \u2705 You've built classification or regression models on tabular data\n",
    "- \u274c You've never worked with time series (or tried and got weird results)\n",
    "- \u274c You don't understand why \"shuffling\" is suddenly dangerous\n",
    "\n",
    "**Time required:** ~30 minutes\n",
    "\n",
    "**After this notebook, you will understand:**\n",
    "1. WHY time series violates standard ML assumptions\n",
    "2. What autocorrelation means and how to read ACF plots\n",
    "3. The three types of data leakage unique to time series\n",
    "4. How to set up your first correct temporal validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Section 1: What You Know vs What Changes\n",
    "\n",
    "Let's start with a clear comparison. Everything in the left column is what you learned for standard ML. The right column shows what's different for time series\u2014and **why**.\n",
    "\n",
    "| Standard ML (Classification/Regression) | Time Series | Why It's Different |\n",
    "|----------------------------------------|-------------|-------------------|\n",
    "| **Data points are independent** | Data points are **dependent** | Today's stock price \u2248 yesterday's |\n",
    "| Each row = different entity (Customer A, B, C) | Each row = **same entity** at different times | All rows are \"Customer A\" over time |\n",
    "| `shuffle=True` protects against ordering bias | `shuffle=True` **creates fake results** | Shuffling lets future data train past |\n",
    "| KFold randomly splits data | KFold is **dangerous** | Random split = \"time travel\" |\n",
    "| MAE tells you model quality | MAE can be **meaningless** | Trivial baselines may achieve tiny MAE |\n",
    "| 80% test accuracy = good model | Need to compare to **naive baseline** | Persistence baseline sets the floor |\n",
    "\n",
    "### The Core Problem: i.i.d. Assumption\n",
    "\n",
    "sklearn assumes your data is **i.i.d.** (independent and identically distributed):\n",
    "- **Independent**: Knowing row 1 tells you nothing about row 2\n",
    "- **Identically distributed**: All rows come from the same distribution\n",
    "\n",
    "Time series **violates independence**:\n",
    "- Knowing today's temperature tells you a LOT about tomorrow's temperature\n",
    "- Knowing this week's stock price tells you a LOT about next week's\n",
    "- This correlation is called **autocorrelation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and configuration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Understanding Autocorrelation (The Key Concept)\n",
    "\n",
    "**Autocorrelation** measures how similar a time series is to itself at different time lags.\n",
    "\n",
    "- **ACF(1)** = correlation between y[t] and y[t-1] (consecutive observations)\n",
    "- **ACF(2)** = correlation between y[t] and y[t-2] (observations 2 steps apart)\n",
    "- And so on...\n",
    "\n",
    "### Intuitive Interpretation\n",
    "\n",
    "| ACF(1) Value | What It Means | Example |\n",
    "|--------------|---------------|----------|\n",
    "| **0.99** | Tomorrow \u2248 today (very sticky) | Interest rates, unemployment |\n",
    "| **0.90** | Strong persistence | Stock volatility, temperature |\n",
    "| **0.50** | Moderate persistence | Some economic indicators |\n",
    "| **0.00** | No autocorrelation (i.i.d.!) | White noise, shuffled data |\n",
    "| **-0.50** | Negative autocorrelation | Alternating patterns |\n",
    "\n",
    "### Why This Matters for Validation\n",
    "\n",
    "When ACF is high:\n",
    "1. Nearby observations are nearly identical\n",
    "2. Shuffling spreads these \"twins\" across train/test\n",
    "3. The model sees answers (future data) disguised as training data\n",
    "4. Validation metrics look amazing... but they're fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ar1(n=500, phi=0.9, sigma=1.0, seed=42):\n",
    "    \"\"\"\n",
    "    Generate AR(1) process: y[t] = phi * y[t-1] + noise\n",
    "    \n",
    "    The parameter 'phi' controls autocorrelation:\n",
    "    - phi = 0.0: white noise (no autocorrelation)\n",
    "    - phi = 0.5: moderate autocorrelation\n",
    "    - phi = 0.9: high autocorrelation (sticky)\n",
    "    - phi = 0.99: very high (like interest rates)\n",
    "    \n",
    "    ACF(1) \u2248 phi for large samples.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.zeros(n)\n",
    "    # Start from stationary distribution\n",
    "    y[0] = rng.normal(0, sigma / np.sqrt(1 - phi**2)) if phi < 1 else 0\n",
    "    for t in range(1, n):\n",
    "        y[t] = phi * y[t-1] + sigma * rng.normal()\n",
    "    return y\n",
    "\n",
    "def compute_acf(series, max_lag=20):\n",
    "    \"\"\"Compute autocorrelation function (ACF) for given lags.\"\"\"\n",
    "    n = len(series)\n",
    "    mean = np.mean(series)\n",
    "    var = np.var(series)\n",
    "    acf = []\n",
    "    for lag in range(max_lag + 1):\n",
    "        if lag == 0:\n",
    "            acf.append(1.0)\n",
    "        else:\n",
    "            cov = np.mean((series[lag:] - mean) * (series[:-lag] - mean))\n",
    "            acf.append(cov / var)\n",
    "    return np.array(acf)\n",
    "\n",
    "# Generate series with different persistence levels\n",
    "series_low = generate_ar1(n=500, phi=0.3, seed=42)\n",
    "series_medium = generate_ar1(n=500, phi=0.7, seed=42)\n",
    "series_high = generate_ar1(n=500, phi=0.95, seed=42)\n",
    "\n",
    "print(\"Generated three series with different autocorrelation levels:\")\n",
    "print(f\"  Low persistence (\u03c6=0.3):    ACF(1) = {np.corrcoef(series_low[1:], series_low[:-1])[0,1]:.3f}\")\n",
    "print(f\"  Medium persistence (\u03c6=0.7): ACF(1) = {np.corrcoef(series_medium[1:], series_medium[:-1])[0,1]:.3f}\")\n",
    "print(f\"  High persistence (\u03c6=0.95):  ACF(1) = {np.corrcoef(series_high[1:], series_high[:-1])[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: What autocorrelation LOOKS like\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "series_list = [series_low, series_medium, series_high]\n",
    "phi_list = [0.3, 0.7, 0.95]\n",
    "titles = ['Low Persistence (\u03c6=0.3)', 'Medium Persistence (\u03c6=0.7)', 'High Persistence (\u03c6=0.95)']\n",
    "\n",
    "# Top row: Time series plots\n",
    "for i, (series, title) in enumerate(zip(series_list, titles)):\n",
    "    ax = axes[0, i]\n",
    "    ax.plot(series[:100], 'steelblue', linewidth=1.5)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    if i == 0:\n",
    "        ax.annotate('Rapid changes\\n(easy to model)', xy=(50, 0), fontsize=9, \n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    elif i == 2:\n",
    "        ax.annotate('Slow drift\\n(hard to beat naive)', xy=(50, 0), fontsize=9,\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "# Bottom row: ACF plots\n",
    "for i, (series, phi) in enumerate(zip(series_list, phi_list)):\n",
    "    ax = axes[1, i]\n",
    "    acf = compute_acf(series, max_lag=15)\n",
    "    lags = np.arange(len(acf))\n",
    "    \n",
    "    # Bar plot for ACF\n",
    "    colors = ['steelblue' if a > 0 else 'salmon' for a in acf]\n",
    "    ax.bar(lags, acf, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    \n",
    "    # Significance bounds (approximate 95% CI)\n",
    "    sig_bound = 1.96 / np.sqrt(len(series))\n",
    "    ax.axhline(y=sig_bound, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.axhline(y=-sig_bound, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Lag')\n",
    "    ax.set_ylabel('ACF')\n",
    "    ax.set_title(f'Autocorrelation Function', fontsize=11)\n",
    "    ax.set_ylim(-0.3, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Understanding Autocorrelation: How \"Sticky\" is Your Data?', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2605 Key Insight: Higher persistence = slower ACF decay = harder to beat naive predictions\")\n",
    "print(\"  - \u03c6=0.3: ACF drops quickly \u2192 models CAN add value\")\n",
    "print(\"  - \u03c6=0.95: ACF stays high \u2192 naive forecast is almost unbeatable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: Scatter plot shows WHY high ACF is a problem\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (series, phi, title) in enumerate(zip(series_list, phi_list, titles)):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(series[:-1], series[1:], alpha=0.5, s=20)\n",
    "    \n",
    "    # Add correlation line\n",
    "    z = np.polyfit(series[:-1], series[1:], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(series.min(), series.max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r-', linewidth=2, label=f'r = {phi:.2f}')\n",
    "    \n",
    "    ax.set_xlabel('y[t]', fontsize=11)\n",
    "    ax.set_ylabel('y[t+1]', fontsize=11)\n",
    "    ax.set_title(f'{title}\\nACF(1) = {np.corrcoef(series[1:], series[:-1])[0,1]:.3f}', fontsize=11)\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Consecutive Values: y[t] vs y[t+1]', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2605 Key Insight: With high ACF, y[t+1] \u2248 y[t]\")\n",
    "print(\"  This means 'predict yesterday' is a very strong baseline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Why Random Shuffling Creates Fake Results\n",
    "\n",
    "Now we'll see the **exact mechanism** by which shuffling breaks validation.\n",
    "\n",
    "### The Setup\n",
    "- We have 500 time points in a series\n",
    "- We want to predict y[t] from lag features: y[t-1], y[t-2], etc.\n",
    "- The series has high autocorrelation (\u03c6=0.9)\n",
    "\n",
    "### What KFold Does (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create high-persistence series\n",
    "series = generate_ar1(n=500, phi=0.9, sigma=1.0, seed=42)\n",
    "\n",
    "# Create lag features\n",
    "def create_lag_features(series, n_lags=3):\n",
    "    \"\"\"Create lag features: y[t-1], y[t-2], ..., y[t-n_lags]\"\"\"\n",
    "    n = len(series)\n",
    "    X = np.column_stack([\n",
    "        np.concatenate([[np.nan]*lag, series[:-lag]]) \n",
    "        for lag in range(1, n_lags + 1)\n",
    "    ])\n",
    "    valid = ~np.isnan(X).any(axis=1)\n",
    "    return X[valid], series[valid], np.where(valid)[0]\n",
    "\n",
    "X, y, valid_indices = create_lag_features(series, n_lags=3)\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Feature columns: y[t-1], y[t-2], y[t-3]\")\n",
    "print(f\"Target: y[t]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE PROBLEM: Visualize what KFold does\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot the series\n",
    "ax.plot(valid_indices, y, 'gray', linewidth=1, alpha=0.5, label='Full series')\n",
    "\n",
    "# Show one fold's train/test split\n",
    "train_idx, test_idx = list(kfold.split(X))[0]\n",
    "\n",
    "# Highlight where test points came from in time\n",
    "test_times = valid_indices[test_idx]\n",
    "train_times = valid_indices[train_idx]\n",
    "\n",
    "ax.scatter(test_times, y[test_idx], c='red', s=30, alpha=0.8, label='Test set (Fold 1)', zorder=5)\n",
    "\n",
    "# Show a few \"problematic\" test points and their neighbors\n",
    "# Find test points that have very close training neighbors\n",
    "problems = []\n",
    "for ti, t_time in enumerate(test_times[:10]):\n",
    "    neighbors = train_times[(train_times >= t_time - 5) & (train_times <= t_time + 5)]\n",
    "    if len(neighbors) > 0:\n",
    "        problems.append((t_time, ti))\n",
    "\n",
    "# Highlight problems\n",
    "for t_time, ti in problems[:5]:\n",
    "    ax.annotate('', xy=(t_time, y[test_idx[ti]]), \n",
    "               xytext=(t_time, y[test_idx[ti]] + 1.5),\n",
    "               arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "ax.set_xlabel('Time Index', fontsize=12)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('KFold SCATTERS Test Points Throughout Time\\n(Red dots are spread across all time periods)', \n",
    "            fontsize=14, fontweight='bold', color='red')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate('Test points at t=50, t=60, t=75...\\nall have training points at\\nt=49, t=59, t=74!\\n\\nModel sees \"answers\" in training.',\n",
    "           xy=(250, y.min()), fontsize=11, color='red', fontweight='bold',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='red', alpha=0.9))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2605 The Problem: Test point at t=100 has y[t-1]=y[99] as a feature.\")\n",
    "print(\"  But with shuffled KFold, y[101] might be in the TRAINING set!\")\n",
    "print(\"  Since y[101] \u2248 y[100] (high ACF), the model 'knows' the answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the problem: How much overlap exists?\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# WRONG: KFold with shuffle\n",
    "kfold_shuffled = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kfold_scores = -cross_val_score(model, X, y, cv=kfold_shuffled, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Also WRONG: KFold without shuffle (still mixes time)\n",
    "kfold_no_shuffle = KFold(n_splits=5, shuffle=False)\n",
    "kfold_ns_scores = -cross_val_score(model, X, y, cv=kfold_no_shuffle, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# CORRECT: Forward-only split (simple version)\n",
    "# Train on first 80%, test on last 20%\n",
    "train_size = int(0.8 * len(y))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "forward_mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "print(\"THE SHOCKING COMPARISON\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"KFold (shuffled) MAE:     {np.mean(kfold_scores):.4f}  <- Looks great!\")\n",
    "print(f\"KFold (no shuffle) MAE:   {np.mean(kfold_ns_scores):.4f}  <- Still wrong!\")\n",
    "print(f\"Forward split MAE:        {forward_mae:.4f}  <- REALITY\")\n",
    "print()\n",
    "print(f\"KFold overestimates performance by {(forward_mae - np.mean(kfold_scores)) / forward_mae * 100:.1f}%\")\n",
    "print()\n",
    "print(\"Both KFold variants are wrong because fold 3's test set\")\n",
    "print(\"contains data that appears BEFORE fold 1's test set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Why Even `shuffle=False` is Wrong\n",
    "\n",
    "You might think: \"I'll just set `shuffle=False`!\"\n",
    "\n",
    "**This is still wrong.** Here's why:\n",
    "\n",
    "```\n",
    "KFold(n_splits=5, shuffle=False) creates:\n",
    "\n",
    "Fold 1: Train on [100-500], Test on [0-100]   <- Model trained on FUTURE!\n",
    "Fold 2: Train on [0-100, 200-500], Test on [100-200]\n",
    "Fold 3: Train on [0-200, 300-500], Test on [200-300]\n",
    "...\n",
    "```\n",
    "\n",
    "In Fold 1, you're training on observations 100-500 to predict observations 0-100. **The model is trained on the future to predict the past!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: The Three Types of Time Series Leakage [T1]\n",
    "\n",
    "Now that you understand WHY shuffling is dangerous, let's categorize the three distinct ways leakage occurs in time series:\n",
    "\n",
    "### Type 1: Data Snooping (Train/Test Contamination)\n",
    "- **What it is**: Future observations in training set\n",
    "- **Caused by**: KFold, random train_test_split\n",
    "- **Detection**: `gate_signal_verification()` from temporalcv\n",
    "\n",
    "### Type 2: Lookahead Bias (Gap Violations)\n",
    "- **What it is**: Target at time t uses information from time t+1, t+2, etc.\n",
    "- **Caused by**: Multi-step forecasts without proper gaps\n",
    "- **Example**: Predicting 12-week ahead returns without 12-week gap\n",
    "- **Detection**: `gate_temporal_boundary()` from temporalcv\n",
    "\n",
    "### Type 3: Feature Leakage (Engineering Errors)\n",
    "- **What it is**: Features computed using future information\n",
    "- **Caused by**: Centered rolling windows, full-series normalization, target encoding\n",
    "- **Example**: `rolling_mean(window=5, center=True)` uses t+1, t+2\n",
    "- **Detection**: Careful code review + `gate_signal_verification()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Type 3: Feature Leakage\n",
    "\n",
    "def create_safe_features(series):\n",
    "    \"\"\"Features that only use PAST information.\"\"\"\n",
    "    n = len(series)\n",
    "    features = {}\n",
    "    \n",
    "    # Safe: Lag features\n",
    "    features['lag_1'] = np.concatenate([[np.nan], series[:-1]])\n",
    "    features['lag_2'] = np.concatenate([[np.nan, np.nan], series[:-2]])\n",
    "    \n",
    "    # Safe: Backward-only rolling mean (uses t-4 to t-1)\n",
    "    rolling = np.full(n, np.nan)\n",
    "    for t in range(4, n):\n",
    "        rolling[t] = np.mean(series[t-4:t])  # Only past!\n",
    "    features['rolling_mean_4'] = rolling\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_leaky_features(series):\n",
    "    \"\"\"Features that LEAK future information!\"\"\"\n",
    "    n = len(series)\n",
    "    features = {}\n",
    "    \n",
    "    # Leaky: Centered rolling mean (uses t-2 to t+2)\n",
    "    rolling = np.full(n, np.nan)\n",
    "    for t in range(2, n - 2):\n",
    "        rolling[t] = np.mean(series[t-2:t+3])  # Includes t+1, t+2!\n",
    "    features['centered_rolling_5'] = rolling\n",
    "    \n",
    "    # Leaky: Z-score using full series mean/std\n",
    "    full_mean = np.mean(series)  # Uses ALL data including future!\n",
    "    full_std = np.std(series)\n",
    "    features['zscore_full'] = (series - full_mean) / full_std\n",
    "    \n",
    "    # Leaky: Percentile rank using full series\n",
    "    from scipy.stats import rankdata\n",
    "    features['pct_rank_full'] = rankdata(series) / len(series)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"SAFE FEATURES (backward-looking only):\")\n",
    "print(\"  - lag_1: y[t-1]\")\n",
    "print(\"  - lag_2: y[t-2]\")\n",
    "print(\"  - rolling_mean_4: mean of y[t-4:t] (excludes current!)\")\n",
    "print()\n",
    "print(\"LEAKY FEATURES (use future information):\")\n",
    "print(\"  - centered_rolling_5: mean of y[t-2:t+3] (includes t+1, t+2!)\")\n",
    "print(\"  - zscore_full: (y - mean(ALL)) / std(ALL)\")\n",
    "print(\"  - pct_rank_full: rank(y) / n using full series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: Show the difference\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Generate series\n",
    "series = generate_ar1(n=200, phi=0.9, seed=42)\n",
    "t_focus = 100  # Focus point\n",
    "\n",
    "# Top left: Safe backward rolling\n",
    "ax = axes[0, 0]\n",
    "ax.plot(series, 'gray', alpha=0.5, linewidth=1)\n",
    "ax.axvline(x=t_focus, color='black', linestyle='--', label='Current time t')\n",
    "ax.axvspan(t_focus-4, t_focus, alpha=0.3, color='green', label='Used for feature')\n",
    "ax.scatter([t_focus], [series[t_focus]], c='blue', s=100, zorder=5)\n",
    "ax.set_title('\u2713 SAFE: Backward Rolling Mean\\nUses only t-4 to t-1', fontsize=11, color='green', fontweight='bold')\n",
    "ax.set_xlabel('Time')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Top right: Leaky centered rolling\n",
    "ax = axes[0, 1]\n",
    "ax.plot(series, 'gray', alpha=0.5, linewidth=1)\n",
    "ax.axvline(x=t_focus, color='black', linestyle='--', label='Current time t')\n",
    "ax.axvspan(t_focus-2, t_focus+3, alpha=0.3, color='red', label='Used for feature')\n",
    "ax.scatter([t_focus], [series[t_focus]], c='blue', s=100, zorder=5)\n",
    "ax.set_title('\u2717 LEAKY: Centered Rolling Mean\\nIncludes t+1 and t+2!', fontsize=11, color='red', fontweight='bold')\n",
    "ax.set_xlabel('Time')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Bottom left: Safe percentile (training only)\n",
    "ax = axes[1, 0]\n",
    "train_end = 150\n",
    "ax.plot(series, 'gray', alpha=0.5, linewidth=1)\n",
    "ax.axvline(x=train_end, color='black', linestyle='--', label='Train/test boundary')\n",
    "ax.axvspan(0, train_end, alpha=0.2, color='green', label='Percentiles from training')\n",
    "ax.set_title('\u2713 SAFE: Percentile from Training Only', fontsize=11, color='green', fontweight='bold')\n",
    "ax.set_xlabel('Time')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Bottom right: Leaky percentile (full series)\n",
    "ax = axes[1, 1]\n",
    "ax.plot(series, 'gray', alpha=0.5, linewidth=1)\n",
    "ax.axvline(x=train_end, color='black', linestyle='--', label='Train/test boundary')\n",
    "ax.axvspan(0, len(series), alpha=0.2, color='red', label='Percentiles from ALL data')\n",
    "ax.set_title('\u2717 LEAKY: Percentile from Full Series\\n(includes test data!)', fontsize=11, color='red', fontweight='bold')\n",
    "ax.set_xlabel('Time')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2605 Rule: Features must only use information available at prediction time.\")\n",
    "print(\"  At time t, you can only use data from times 0, 1, 2, ..., t-1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Your First Correct Temporal Validation\n",
    "\n",
    "Now let's set up validation the RIGHT way using **Walk-Forward Cross-Validation**.\n",
    "\n",
    "### The Key Principles\n",
    "\n",
    "1. **Training data must PRECEDE test data** (no exceptions!)\n",
    "2. **Gap between train and test** equals your forecast horizon\n",
    "3. **Use expanding or sliding windows** to simulate production\n",
    "\n",
    "### Walk-Forward Visualization\n",
    "\n",
    "```\n",
    "Fold 1: [=====TRAIN=====][GAP][TEST]\n",
    "Fold 2: [=======TRAIN=======][GAP][TEST]\n",
    "Fold 3: [=========TRAIN=========][GAP][TEST]\n",
    "Fold 4: [===========TRAIN===========][GAP][TEST]\n",
    "```\n",
    "\n",
    "The training window expands (or slides) forward in time. Test always comes AFTER train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import temporalcv for proper validation\n",
    "from temporalcv.cv import WalkForwardCV\n",
    "\n",
    "# Generate our data\n",
    "series = generate_ar1(n=500, phi=0.9, sigma=1.0, seed=42)\n",
    "X, y, valid_indices = create_lag_features(series, n_lags=3)\n",
    "\n",
    "# Set up proper walk-forward CV\n",
    "cv = WalkForwardCV(\n",
    "    n_splits=5,\n",
    "    window_type='expanding',  # Training window grows over time\n",
    "    horizon=1, extra_gap=0,                    # horizon=1, extra_gap=0 for 1-step forecasting (extra_gap=h for h-step)\n",
    "    test_size=50,             # Test on 50 observations per fold\n",
    ")\n",
    "\n",
    "# Visualize the splits\n",
    "print(\"Walk-Forward CV Splits:\")\n",
    "print(\"=\" * 60)\n",
    "for info in cv.get_split_info(X):\n",
    "    train_bar = '\u2588' * (info.train_size // 10)\n",
    "    gap_bar = '\u2591' * (info.gap)\n",
    "    test_bar = '\u2593' * (info.test_size // 10)\n",
    "    print(f\"Fold {info.split_idx+1}: Train[{info.train_start}:{info.train_end}] \u2192 \"\n",
    "          f\"Test[{info.test_start}:{info.test_end}]  \"\n",
    "          f\"(train={info.train_size}, test={info.test_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: KFold vs WalkForward\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Top: KFold (WRONG)\n",
    "ax = axes[0]\n",
    "ax.set_title('\u274c KFold: Test Points Scattered in Time (WRONG)', fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X)):\n",
    "    ax.scatter(valid_indices[train_idx], [fold]*len(train_idx), c='steelblue', s=5, alpha=0.5, marker='s')\n",
    "    ax.scatter(valid_indices[test_idx], [fold]*len(test_idx), c='salmon', s=10, alpha=0.8, marker='o')\n",
    "\n",
    "ax.set_xlabel('Time Index')\n",
    "ax.set_ylabel('Fold')\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])\n",
    "\n",
    "# Bottom: WalkForward (CORRECT)\n",
    "ax = axes[1]\n",
    "ax.set_title('\u2713 WalkForward: Test Always After Train (CORRECT)', fontsize=12, fontweight='bold', color='green')\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(X)):\n",
    "    train_times = valid_indices[train_idx]\n",
    "    test_times = valid_indices[test_idx]\n",
    "    \n",
    "    # Draw as bars for clarity\n",
    "    ax.barh(fold, train_times.max() - train_times.min(), left=train_times.min(), \n",
    "           height=0.6, color='steelblue', alpha=0.7, label='Train' if fold==0 else '')\n",
    "    ax.barh(fold, test_times.max() - test_times.min(), left=test_times.min(), \n",
    "           height=0.6, color='salmon', alpha=0.9, label='Test' if fold==0 else '')\n",
    "    \n",
    "    # Draw boundary\n",
    "    ax.axvline(x=train_times.max(), ymin=(fold-0.3)/5, ymax=(fold+0.7)/5, \n",
    "              color='black', linestyle='--', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Time Index')\n",
    "ax.set_ylabel('Fold')\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run proper validation\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "# Collect scores from walk-forward CV\n",
    "wf_scores = []\n",
    "for train_idx, test_idx in cv.split(X):\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    preds = model.predict(X[test_idx])\n",
    "    mae = mean_absolute_error(y[test_idx], preds)\n",
    "    wf_scores.append(mae)\n",
    "\n",
    "# Compare to KFold\n",
    "kfold_scores = -cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Compare to persistence baseline\n",
    "persistence_errors = []\n",
    "for train_idx, test_idx in cv.split(X):\n",
    "    persistence_preds = X[test_idx, 0]  # y[t-1]\n",
    "    mae = mean_absolute_error(y[test_idx], persistence_preds)\n",
    "    persistence_errors.append(mae)\n",
    "\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"KFold MAE:           {np.mean(kfold_scores):.4f} \u00b1 {np.std(kfold_scores):.4f}  <- FAKE\")\n",
    "print(f\"Walk-Forward MAE:    {np.mean(wf_scores):.4f} \u00b1 {np.std(wf_scores):.4f}  <- REAL\")\n",
    "print(f\"Persistence MAE:     {np.mean(persistence_errors):.4f} \u00b1 {np.std(persistence_errors):.4f}  <- BASELINE\")\n",
    "print()\n",
    "improvement = (np.mean(persistence_errors) - np.mean(wf_scores)) / np.mean(persistence_errors) * 100\n",
    "print(f\"Model improvement over persistence: {improvement:.1f}%\")\n",
    "print()\n",
    "if improvement > 5:\n",
    "    print(\"\u2713 Model adds value beyond naive baseline!\")\n",
    "else:\n",
    "    print(\"\u26a0 Model barely beats persistence. Consider simpler approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What You've Learned\n",
    "\n",
    "### 1. Time Series Violates i.i.d. [T1]\n",
    "- Observations are NOT independent\n",
    "- Autocorrelation (ACF) measures this dependence\n",
    "- High ACF = sticky data = harder to beat naive\n",
    "\n",
    "### 2. Shuffling Creates Fake Results [T1]\n",
    "- KFold with `shuffle=True` causes temporal leakage\n",
    "- Even `shuffle=False` is wrong (future trains past)\n",
    "- Walk-Forward CV is the correct approach\n",
    "\n",
    "### 3. Three Types of Leakage [T1]\n",
    "- **Data snooping**: Future in training set\n",
    "- **Lookahead bias**: Gap < forecast horizon\n",
    "- **Feature leakage**: Features use future information\n",
    "\n",
    "### 4. How to Read ACF Plots\n",
    "- ACF(1) > 0.9: High persistence, naive is strong\n",
    "- ACF decays slowly: Hard to beat persistence\n",
    "- ACF decays fast: Models can add value\n",
    "\n",
    "### 5. The Correct Validation Setup\n",
    "```python\n",
    "from temporalcv.cv import WalkForwardCV\n",
    "\n",
    "cv = WalkForwardCV(\n",
    "    n_splits=5,\n",
    "    window_type='expanding',\n",
    "    extra_gap=horizon,  # gap >= forecast horizon\n",
    "    test_size=50,\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue your learning path:\n",
    "\n",
    "1. **00b_real_world_motivation.ipynb**: See realistic synthetic examples (Treasury rates, stock returns)\n",
    "2. **01_why_temporal_cv.ipynb**: Deep dive into validation gates and detection\n",
    "3. **Feature Engineering Safety Guide**: Decision tree for safe vs dangerous features\n",
    "\n",
    "---\n",
    "\n",
    "*\"The most common mistake in time series ML is treating it like regular ML. Once you understand autocorrelation, everything else follows.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}