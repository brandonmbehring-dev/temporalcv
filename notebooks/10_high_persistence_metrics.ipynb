{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 10: High-Persistence Metrics\n\n**MC-SS, Move-Conditional Evaluation, and Theil's U**\n\n---\n\n## ðŸš¨ If You Know sklearn But Not High-Persistence Data, Read This First\n\n**What you already know (from standard ML)**:\n- Lower MAE = better model\n- 10% improvement over baseline = meaningful progress\n- If your model beats the baseline, you've learned something useful\n\n**What's different with high-persistence time series** (like Treasury rates, unemployment, GDP):\n\n| Metric | Typical Value | Interpretation |\n|--------|---------------|----------------|\n| Model MAE | 0.0012 | Looks amazing! |\n| Persistence MAE | 0.0013 | Also amazing... |\n| Improvement | 8% | Sounds good! |\n\n**The problem**: Both models are essentially predicting \"no change.\"\n\n```python\n# What's really happening:\ny[t]   = 5.001\ny[t+1] = 5.002  # Barely moved!\n\n# Persistence predicts: y_hat[t+1] = 5.001 (yesterday's value)\n# Your model predicts:  y_hat[t+1] = 5.0015 (split the difference)\n# Both are \"right\" because the series barely moved!\n```\n\n**The question we should ask**: When the series *actually moves*, does your model predict the move?\n\n**The fix**: Move-Conditional Skill Score (MC-SS) â€” evaluate only on significant moves.\n\n---\n\n### Why 20% Improvement = \"Too Good To Be True\" [T3]\n\nFrom 7+ post-mortems in production forecasting:\n- **5-10% improvement**: Plausible with good feature engineering\n- **10-20%**: Unusual but possible; warrants investigation  \n- **>20%**: Almost always indicates leakage or bug\n\nThis threshold caught:\n- BUG-001: Lag feature leakage (reported 35% improvement)\n- BUG-005: Regime computation lookahead (reported 28% improvement)\n- BUG-009: Internal-only validation (reported 42% improvement)\n\n---\n\n## What You'll Learn\n\n1. **Why standard metrics fail on persistent data** â€” The persistence paradox\n2. **Move-conditional metrics** â€” MC-SS isolates skill on significant events [T2]\n3. **Theil's U statistic** â€” Scale-free comparison to naive baseline [T1]\n\n**Prerequisites**: Notebooks 01-04 (Foundation tier)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: The Persistence Paradox\n",
    "\n",
    "Consider a highly persistent time series (like Treasury rates):\n",
    "\n",
    "```\n",
    "Model MAE:       0.0012\n",
    "Persistence MAE: 0.0013\n",
    "Improvement:     8%\n",
    "```\n",
    "\n",
    "**Looks good, right?** But wait...\n",
    "\n",
    "Both are essentially \"predicting no change\" because:\n",
    "- Series barely moves most of the time\n",
    "- Model learned to copy y[t-1] (just like persistence)\n",
    "- Small differences are just noise\n",
    "\n",
    "**The paradox**: Standard MAE can't distinguish genuine skill from lucky noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from temporalcv.persistence import (\n",
    "    compute_move_threshold,\n",
    "    compute_move_conditional_metrics,\n",
    "    compute_direction_accuracy,\n",
    ")\n",
    "from temporalcv.metrics import compute_theils_u\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"temporalcv high-persistence metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HIGH-PERSISTENCE data (phi = 0.98)\n",
    "def generate_ar1(n: int, phi: float, sigma: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"Generate AR(1) process: y[t] = phi * y[t-1] + epsilon[t]\"\"\"\n",
    "    y = np.zeros(n)\n",
    "    y[0] = 5.0  # Start at a level (like an interest rate)\n",
    "    for t in range(1, n):\n",
    "        y[t] = phi * y[t - 1] + np.random.normal(0, sigma)\n",
    "    return y\n",
    "\n",
    "# High persistence like Treasury rates\n",
    "n = 500\n",
    "y = generate_ar1(n, phi=0.98, sigma=0.02)\n",
    "\n",
    "# Create features and split\n",
    "X = np.column_stack([y[:-2], y[1:-1]])\n",
    "y_target = y[2:]\n",
    "\n",
    "train_size = int(len(y_target) * 0.7)\n",
    "X_train, y_train = X[:train_size], y_target[:train_size]\n",
    "X_test, y_test = X[train_size:], y_target[train_size:]\n",
    "\n",
    "print(f\"Data range: {y.min():.3f} to {y.max():.3f}\")\n",
    "print(f\"Daily changes std: {np.std(np.diff(y)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and compute predictions\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Persistence baseline: y[t-1]\n",
    "persistence = X_test[:, -1]\n",
    "\n",
    "# Standard MAE comparison\n",
    "model_mae = np.mean(np.abs(y_test - predictions))\n",
    "persistence_mae = np.mean(np.abs(y_test - persistence))\n",
    "improvement = 1 - model_mae / persistence_mae\n",
    "\n",
    "print(\"Standard MAE Comparison:\")\n",
    "print(f\"  Model MAE:       {model_mae:.6f}\")\n",
    "print(f\"  Persistence MAE: {persistence_mae:.6f}\")\n",
    "print(f\"  Improvement:     {improvement:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The problem**: Both MAE values are tiny because the series barely moves.\n",
    "\n",
    "**We need metrics that focus on *when the series actually moves*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Move Threshold Definition [T2]\n",
    "\n",
    "A \"move\" is defined as a change exceeding the **70th percentile** of historical |changes|.\n",
    "\n",
    "This means:\n",
    "- ~30% of periods are \"moves\" (UP or DOWN)\n",
    "- ~70% are \"flat\" (noise around no change)\n",
    "\n",
    "**Critical**: The threshold MUST be computed from **training data only**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute changes (we evaluate on CHANGES, not levels)\n",
    "train_changes = np.diff(y_train)\n",
    "test_changes = np.diff(y_test)\n",
    "pred_changes = np.diff(predictions)\n",
    "\n",
    "# Compute threshold from TRAINING data only!\n",
    "threshold = compute_move_threshold(train_changes, percentile=70.0)\n",
    "\n",
    "print(f\"Move threshold (70th percentile): {threshold:.6f}\")\n",
    "print(f\"Computed from training data: n={len(train_changes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify test changes\n",
    "n_up = np.sum(test_changes > threshold)\n",
    "n_down = np.sum(test_changes < -threshold)\n",
    "n_flat = np.sum(np.abs(test_changes) <= threshold)\n",
    "\n",
    "print(f\"\\nTest data classification:\")\n",
    "print(f\"  UP:   {n_up} ({n_up/len(test_changes):.1%})\")\n",
    "print(f\"  DOWN: {n_down} ({n_down/len(test_changes):.1%})\")\n",
    "print(f\"  FLAT: {n_flat} ({n_flat/len(test_changes):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. MC-SS (Move-Conditional Skill Score) [T2]\n",
    "\n",
    "MC-SS measures skill **only on periods when the series actually moved**.\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "MC-SS = 1 - (model_mae_on_moves / persistence_mae_on_moves)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- MC-SS = 0: Model equals persistence on moves\n",
    "- MC-SS > 0: Model beats persistence on moves\n",
    "- MC-SS < 0: Model worse than persistence on moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute move-conditional metrics\n",
    "mc = compute_move_conditional_metrics(\n",
    "    predictions=pred_changes,\n",
    "    actuals=test_changes,\n",
    "    threshold=threshold,  # From training data!\n",
    ")\n",
    "\n",
    "print(\"Move-Conditional Metrics:\")\n",
    "print(f\"  MAE on UP moves:   {mc.mae_up:.6f}\")\n",
    "print(f\"  MAE on DOWN moves: {mc.mae_down:.6f}\")\n",
    "print(f\"  MAE on FLAT:       {mc.mae_flat:.6f}\")\n",
    "print(f\"\\n  MC-SS: {mc.skill_score:.3f}\")\n",
    "print(f\"  Reliable? {mc.is_reliable} (n_up={mc.n_up}, n_down={mc.n_down})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability Check\n",
    "\n",
    "MC-SS requires sufficient samples in each direction:\n",
    "- `is_reliable = True` if n_up >= 10 AND n_down >= 10\n",
    "- If unreliable, treat MC-SS with caution [T3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoveConditionalResult provides summary\n",
    "print(f\"\\nTotal samples: {mc.n_total}\")\n",
    "print(f\"Move fraction: {mc.move_fraction:.1%} (should be ~30%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Direction Accuracy\n",
    "\n",
    "Did the model predict the **correct direction** of change?\n",
    "\n",
    "**Two modes:**\n",
    "\n",
    "| Mode | Classes | Use Case |\n",
    "|------|---------|----------|\n",
    "| 2-class | UP/DOWN | General direction accuracy |\n",
    "| 3-class | UP/DOWN/FLAT | Fair comparison to persistence |\n",
    "\n",
    "**Why 3-class matters**: Persistence always predicts FLAT (zero change).\n",
    "In 2-class, persistence gets 0% accuracy (trivially). \n",
    "In 3-class, persistence gets credit when actual is FLAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-class direction accuracy (sign-based)\n",
    "acc_2class = compute_direction_accuracy(pred_changes, test_changes)\n",
    "\n",
    "# 3-class direction accuracy (with threshold)\n",
    "acc_3class = compute_direction_accuracy(\n",
    "    pred_changes, test_changes, move_threshold=threshold\n",
    ")\n",
    "\n",
    "print(\"Direction Accuracy:\")\n",
    "print(f\"  2-class (sign): {acc_2class:.1%}\")\n",
    "print(f\"  3-class (UP/DOWN/FLAT): {acc_3class:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to persistence (predicts 0 = FLAT)\n",
    "persistence_changes = np.zeros_like(test_changes)  # Persistence predicts no change\n",
    "\n",
    "pers_acc_2class = compute_direction_accuracy(persistence_changes, test_changes)\n",
    "pers_acc_3class = compute_direction_accuracy(\n",
    "    persistence_changes, test_changes, move_threshold=threshold\n",
    ")\n",
    "\n",
    "print(\"\\nPersistence Direction Accuracy:\")\n",
    "print(f\"  2-class: {pers_acc_2class:.1%} (trivially 0%)\")\n",
    "print(f\"  3-class: {pers_acc_3class:.1%} (credit for FLAT periods)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Theil's U Statistic [T1]\n",
    "\n",
    "Theil's U compares model RMSE to naive (persistence) RMSE:\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "U = RMSE(model) / RMSE(naive)\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "- U < 1: Model beats naive\n",
    "- U = 1: Model equals naive\n",
    "- U > 1: Model worse than naive\n",
    "\n",
    "**Citation**: Theil (1966)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Theil's U\n",
    "theils_u = compute_theils_u(predictions, y_test)\n",
    "\n",
    "print(f\"Theil's U: {theils_u:.4f}\")\n",
    "if theils_u < 1:\n",
    "    print(f\"  Model beats persistence by {(1 - theils_u)*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"  Model is {(theils_u - 1)*100:.1f}% worse than persistence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Complete Evaluation Template\n",
    "\n",
    "Here's a production-ready function for high-persistence data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_high_persistence(\n",
    "    predictions: np.ndarray,\n",
    "    actuals: np.ndarray,\n",
    "    train_actuals: np.ndarray,\n",
    "    threshold_percentile: float = 70.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Complete evaluation for high-persistence time series.\n",
    "    \n",
    "    Returns dict with standard metrics, move-conditional metrics,\n",
    "    direction accuracy, and Theil's U.\n",
    "    \"\"\"\n",
    "    # Standard metrics\n",
    "    mae = float(np.mean(np.abs(actuals - predictions)))\n",
    "    \n",
    "    # Persistence baseline\n",
    "    persistence_mae = float(np.mean(np.abs(np.diff(actuals))))\n",
    "    \n",
    "    # Move threshold from TRAINING data\n",
    "    train_changes = np.diff(train_actuals)\n",
    "    threshold = compute_move_threshold(train_changes, percentile=threshold_percentile)\n",
    "    \n",
    "    # Move-conditional metrics\n",
    "    pred_changes = np.diff(predictions)\n",
    "    test_changes = np.diff(actuals)\n",
    "    \n",
    "    mc = compute_move_conditional_metrics(\n",
    "        pred_changes, test_changes, threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Direction accuracy (3-class)\n",
    "    dir_acc = compute_direction_accuracy(\n",
    "        pred_changes, test_changes, move_threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Theil's U\n",
    "    theils_u = compute_theils_u(predictions, actuals)\n",
    "    \n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"persistence_mae\": persistence_mae,\n",
    "        \"improvement\": 1 - mae / persistence_mae,\n",
    "        \"mc_ss\": mc.skill_score,\n",
    "        \"mc_reliable\": mc.is_reliable,\n",
    "        \"n_up\": mc.n_up,\n",
    "        \"n_down\": mc.n_down,\n",
    "        \"direction_accuracy\": dir_acc,\n",
    "        \"theils_u\": theils_u,\n",
    "        \"move_threshold\": threshold,\n",
    "    }\n",
    "\n",
    "# Test the template\n",
    "results = evaluate_high_persistence(predictions, y_test, y_train)\n",
    "\n",
    "print(\"Complete Evaluation:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pitfall Section: Common Mistakes\n",
    "\n",
    "### Pitfall 1: Threshold from Full Series\n",
    "\n",
    "```python\n",
    "# WRONG: Compute threshold from all data (leaks future!)\n",
    "all_changes = np.diff(y_all)\n",
    "threshold = compute_move_threshold(all_changes)  # BUG!\n",
    "\n",
    "# RIGHT: Compute from training data only\n",
    "train_changes = np.diff(y_train)\n",
    "threshold = compute_move_threshold(train_changes)  # Safe\n",
    "```\n",
    "\n",
    "**Why it matters**: Using test data to define \"moves\" is lookahead bias (BUG-003).\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 2: Using Levels Instead of Changes\n",
    "\n",
    "```python\n",
    "# WRONG: Pass raw levels to persistence metrics\n",
    "mc = compute_move_conditional_metrics(predictions, actuals)  # ERROR!\n",
    "\n",
    "# RIGHT: Pass changes (differences)\n",
    "pred_changes = np.diff(predictions)\n",
    "actual_changes = np.diff(actuals)\n",
    "mc = compute_move_conditional_metrics(pred_changes, actual_changes)\n",
    "```\n",
    "\n",
    "**Why it matters**: Persistence metrics assume \"predicting zero change\" as baseline.\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 3: Ignoring Reliability Flag\n",
    "\n",
    "```python\n",
    "# WRONG: Trust MC-SS without checking reliability\n",
    "print(f\"MC-SS: {mc.skill_score:.3f}\")  # May be unreliable!\n",
    "\n",
    "# RIGHT: Check reliability first\n",
    "if mc.is_reliable:\n",
    "    print(f\"MC-SS: {mc.skill_score:.3f}\")\n",
    "else:\n",
    "    print(f\"Warning: Only {mc.n_up} UP and {mc.n_down} DOWN samples\")\n",
    "```\n",
    "\n",
    "**Why it matters**: n < 10 per direction â†’ high variance â†’ meaningless score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "```\n",
    "â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "1. Standard MAE fails on high-persistence data\n",
    "   - Series barely moves â†’ all models look similar\n",
    "   - Need move-conditional evaluation\n",
    "\n",
    "2. Move threshold = 70th percentile [T2]\n",
    "   - ~30% moves, ~70% flat\n",
    "   - MUST be computed from training data only\n",
    "\n",
    "3. MC-SS isolates skill on significant moves\n",
    "   - Check is_reliable flag (n >= 10 per direction)\n",
    "   - Positive MC-SS = genuine forecasting skill\n",
    "\n",
    "4. Theil's U provides scale-free comparison [T1]\n",
    "   - U < 1 means model beats persistence\n",
    "   - Works on levels (not changes)\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "| Notebook | Topic |\n",
    "|----------|-------|\n",
    "| **11** | Conformal prediction for uncertainty quantification |\n",
    "| 12 | Regime-stratified evaluation (capstone) |\n",
    "\n",
    "---\n",
    "\n",
    "**You've learned**: How to properly evaluate forecasting models on high-persistence data using move-conditional metrics that isolate genuine skill from noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}