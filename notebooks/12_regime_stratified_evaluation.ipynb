{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 12: Regime-Stratified Evaluation\n\n**Capstone: Complete Validation Combining All Techniques**\n\n---\n\n## ðŸš¨ If You've Made It This Far: The Final Trap\n\n**What you've learned so far**:\n- KFold leaks future data â†’ Use WalkForwardCV\n- Gap enforcement prevents target overlap â†’ gap >= horizon\n- Persistence is hard to beat on sticky data â†’ Use MASE, MC-SS\n- Shuffled target catches feature leakage â†’ Run before trusting results\n- Statistical tests need HAC variance â†’ Use dm_test, not ttest\n\n**The final trap**: **Aggregate metrics hide regime-specific failures.**\n\nConsider this scenario:\n```\nOverall MAE: 0.045\nImprovement over persistence: 12%\nShuffled target: PASS\nDM test: p = 0.03 (significant!)\n\nConclusion: Model is great, ship it! âœ“\n```\n\n**But then you stratify by volatility regime...**\n\n```\nLOW volatility:    Improvement = +25%  â† Great!\nMEDIUM volatility: Improvement = +15%  â† Good\nHIGH volatility:   Improvement = -5%   â† FAILURE!\n```\n\n**The model fails exactly when it matters most** â€” during high volatility (crisis periods, market stress, economic transitions).\n\nThe overall metrics looked good because:\n- Most data points are LOW/MEDIUM volatility (boring periods)\n- HIGH volatility periods are rare but critical\n- Averaging hides the failure\n\n**This notebook teaches stratified evaluation** â€” the final defense against hidden model failures.\n\n---\n\n## What You'll Learn\n\n1. **Why aggregate metrics hide failures** â€” Regime-specific issues masked by overall performance\n2. **Stratified validation pipeline** â€” `run_gates_stratified()` for regime-aware checks\n3. **Complete evaluation template** â€” Combining DM tests, MC-SS, and conformal prediction\n\n**Prerequisites**: Notebooks 01-11 (all prior notebooks)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Aggregate Metrics Hide Regime-Specific Failures\n",
    "\n",
    "Consider this validation result:\n",
    "\n",
    "```\n",
    "Overall MAE: 0.045\n",
    "Overall improvement over persistence: 12%\n",
    "Status: PASS\n",
    "```\n",
    "\n",
    "**But what if we stratify by regime?**\n",
    "\n",
    "```\n",
    "LOW volatility:    MAE = 0.020, improvement = 25%  â† Great!\n",
    "MEDIUM volatility: MAE = 0.040, improvement = 15%  â† Good\n",
    "HIGH volatility:   MAE = 0.095, improvement = -5%  â† FAILURE!\n",
    "```\n",
    "\n",
    "**The model fails exactly when it matters most** â€” during high volatility periods.\n",
    "\n",
    "This notebook teaches you to detect such hidden failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from temporalcv.gates import (\n",
    "    gate_shuffled_target,\n",
    "    gate_suspicious_improvement,\n",
    "    run_gates,\n",
    "    run_gates_stratified,\n",
    "    GateStatus,\n",
    ")\n",
    "from temporalcv.statistical_tests import dm_test, pt_test\n",
    "from temporalcv.persistence import compute_move_conditional_metrics, compute_move_threshold\n",
    "from temporalcv.conformal import walk_forward_conformal\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"temporalcv capstone: regime-stratified evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with distinct volatility regimes\n",
    "def generate_ar1(n: int, phi: float, sigma: float) -> np.ndarray:\n",
    "    y = np.zeros(n)\n",
    "    y[0] = np.random.normal(0, sigma)\n",
    "    for t in range(1, n):\n",
    "        y[t] = phi * y[t - 1] + np.random.normal(0, sigma)\n",
    "    return y\n",
    "\n",
    "# LOW volatility regime (periods 0-150)\n",
    "y_low = generate_ar1(150, phi=0.9, sigma=0.05)\n",
    "\n",
    "# HIGH volatility regime (periods 150-350)\n",
    "y_high = generate_ar1(200, phi=0.9, sigma=0.25)\n",
    "y_high = y_high + y_low[-1]\n",
    "\n",
    "# MEDIUM volatility regime (periods 350-500)\n",
    "y_med = generate_ar1(150, phi=0.9, sigma=0.12)\n",
    "y_med = y_med + y_high[-1]\n",
    "\n",
    "# Combine\n",
    "y = np.concatenate([y_low, y_high, y_med])\n",
    "regime_labels = np.array([\"LOW\"] * 150 + [\"HIGH\"] * 200 + [\"MEDIUM\"] * 150)\n",
    "\n",
    "# Create features\n",
    "X = np.column_stack([y[:-2], y[1:-1]])\n",
    "y_target = y[2:]\n",
    "regimes = regime_labels[2:]\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(len(y_target) * 0.7)\n",
    "X_train, y_train = X[:train_size], y_target[:train_size]\n",
    "X_test, y_test = X[train_size:], y_target[train_size:]\n",
    "regimes_test = regimes[train_size:]\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"\\nTest regime distribution:\")\n",
    "for r in [\"LOW\", \"MEDIUM\", \"HIGH\"]:\n",
    "    n = np.sum(regimes_test == r)\n",
    "    print(f\"  {r}: {n} ({n/len(regimes_test):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and get predictions\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute overall metrics\n",
    "overall_mae = np.mean(np.abs(y_test - predictions))\n",
    "persistence_mae = np.mean(np.abs(np.diff(y_test)))\n",
    "improvement = 1 - overall_mae / persistence_mae\n",
    "\n",
    "print(f\"Overall MAE: {overall_mae:.4f}\")\n",
    "print(f\"Persistence MAE: {persistence_mae:.4f}\")\n",
    "print(f\"Overall improvement: {improvement:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks good overall!** But let's stratify by regime..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Per-Regime Metrics\n",
    "\n",
    "First, let's compute metrics stratified by volatility regime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Per-Regime Performance:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for regime in [\"LOW\", \"MEDIUM\", \"HIGH\"]:\n",
    "    mask = regimes_test == regime\n",
    "    \n",
    "    if mask.sum() < 10:\n",
    "        print(f\"{regime}: Insufficient samples (n={mask.sum()})\")\n",
    "        continue\n",
    "    \n",
    "    regime_actuals = y_test[mask]\n",
    "    regime_preds = predictions[mask]\n",
    "    \n",
    "    # MAE\n",
    "    regime_mae = np.mean(np.abs(regime_actuals - regime_preds))\n",
    "    \n",
    "    # Persistence baseline (for this regime)\n",
    "    regime_persistence = np.mean(np.abs(np.diff(regime_actuals)))\n",
    "    \n",
    "    # Improvement\n",
    "    if regime_persistence > 0:\n",
    "        regime_improvement = 1 - regime_mae / regime_persistence\n",
    "    else:\n",
    "        regime_improvement = 0\n",
    "    \n",
    "    status = \"PASS\" if regime_improvement > 0 else \"WARN\" if regime_improvement > -0.1 else \"HALT\"\n",
    "    \n",
    "    print(f\"\\n{regime} (n={mask.sum()}):\")\n",
    "    print(f\"  MAE: {regime_mae:.4f}\")\n",
    "    print(f\"  Persistence MAE: {regime_persistence:.4f}\")\n",
    "    print(f\"  Improvement: {regime_improvement:.1%}\")\n",
    "    print(f\"  Status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: The model may fail in specific regimes while passing overall.\n",
    "\n",
    "This is why we need **stratified validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Stratified Validation with `run_gates_stratified()`\n",
    "\n",
    "`run_gates_stratified()` runs validation both:\n",
    "- **Overall**: All gates on full dataset\n",
    "- **Per-regime**: Numeric gates on each regime separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, compute overall gates\n",
    "overall_gates = [\n",
    "    gate_shuffled_target(\n",
    "        model, X, y_target,\n",
    "        method=\"effect_size\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    gate_suspicious_improvement(\n",
    "        model_metric=overall_mae,\n",
    "        baseline_metric=persistence_mae\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run stratified validation\n",
    "stratified_report = run_gates_stratified(\n",
    "    overall_gates=overall_gates,\n",
    "    actuals=y_test,\n",
    "    predictions=predictions,\n",
    "    regimes=regimes_test,\n",
    "    min_n_per_regime=10\n",
    ")\n",
    "\n",
    "print(stratified_report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Detect Volatility Regimes\n",
    "\n",
    "If you don't have regime labels, use `regimes=\"auto\"` to classify based on rolling volatility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect regimes from actuals\n",
    "auto_report = run_gates_stratified(\n",
    "    overall_gates=overall_gates,\n",
    "    actuals=y_test,\n",
    "    predictions=predictions,\n",
    "    regimes=\"auto\",  # Auto-classify volatility\n",
    "    volatility_window=13,  # ~1 quarter [T3]\n",
    "    min_n_per_regime=10\n",
    ")\n",
    "\n",
    "print(\"\\nAuto-detected regime counts:\")\n",
    "for regime, count in auto_report.regime_counts.items():\n",
    "    print(f\"  {regime}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Per-Regime Statistical Tests\n",
    "\n",
    "Apply the DM test and MC-SS per regime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute move threshold from training data\n",
    "train_changes = np.diff(y_train)\n",
    "threshold = compute_move_threshold(train_changes, percentile=70.0)\n",
    "\n",
    "print(\"Per-Regime Statistical Tests:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for regime in [\"LOW\", \"MEDIUM\", \"HIGH\"]:\n",
    "    mask = regimes_test == regime\n",
    "    if mask.sum() < 30:  # DM test needs n >= 30\n",
    "        print(f\"\\n{regime}: Insufficient samples for DM test (n={mask.sum()})\")\n",
    "        continue\n",
    "    \n",
    "    regime_actuals = y_test[mask]\n",
    "    regime_preds = predictions[mask]\n",
    "    persistence = X_test[mask, -1]  # y[t-1]\n",
    "    \n",
    "    # DM test: model vs persistence\n",
    "    model_errors = regime_actuals - regime_preds\n",
    "    persistence_errors = regime_actuals - persistence\n",
    "    \n",
    "    try:\n",
    "        dm = dm_test(model_errors, persistence_errors, h=1)\n",
    "        dm_result = f\"DM={dm.statistic:.2f}, p={dm.pvalue:.3f}\"\n",
    "    except ValueError as e:\n",
    "        dm_result = f\"Error: {e}\"\n",
    "    \n",
    "    # MC-SS\n",
    "    pred_changes = np.diff(regime_preds)\n",
    "    actual_changes = np.diff(regime_actuals)\n",
    "    \n",
    "    mc = compute_move_conditional_metrics(pred_changes, actual_changes, threshold=threshold)\n",
    "    \n",
    "    print(f\"\\n{regime} (n={mask.sum()}):\")\n",
    "    print(f\"  {dm_result}\")\n",
    "    print(f\"  MC-SS: {mc.skill_score:.3f} (reliable: {mc.is_reliable})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Per-Regime Conformal Prediction\n",
    "\n",
    "Check if prediction intervals maintain coverage across regimes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall conformal prediction\n",
    "intervals, quality = walk_forward_conformal(\n",
    "    predictions, y_test,\n",
    "    calibration_fraction=0.3,\n",
    "    alpha=0.10\n",
    ")\n",
    "\n",
    "print(f\"Overall coverage: {quality['coverage']:.1%} (target: 90%)\")\n",
    "print(f\"Mean interval width: {quality['mean_width']:.4f}\")\n",
    "\n",
    "# Per-regime coverage\n",
    "cal_size = int(len(y_test) * 0.3)\n",
    "holdout_actuals = y_test[cal_size:]\n",
    "holdout_regimes = regimes_test[cal_size:]\n",
    "\n",
    "print(\"\\nPer-Regime Coverage:\")\n",
    "for regime in [\"LOW\", \"MEDIUM\", \"HIGH\"]:\n",
    "    mask = holdout_regimes == regime\n",
    "    if mask.sum() < 10:\n",
    "        continue\n",
    "    \n",
    "    regime_coverage = intervals.coverage(holdout_actuals[mask])\n",
    "    print(f\"  {regime}: {regime_coverage:.1%} (n={mask.sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Complete Evaluation Template\n",
    "\n",
    "Here's a production-ready function combining everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CompleteEvaluation:\n",
    "    \"\"\"Complete model evaluation results.\"\"\"\n",
    "    overall_status: str\n",
    "    overall_mae: float\n",
    "    overall_improvement: float\n",
    "    dm_pvalue: Optional[float]\n",
    "    mc_ss: float\n",
    "    mc_reliable: bool\n",
    "    conformal_coverage: float\n",
    "    regime_results: Dict[str, Dict[str, Any]]\n",
    "    worst_regime: str\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        lines = [\n",
    "            \"=\"*60,\n",
    "            \"COMPLETE MODEL EVALUATION\",\n",
    "            \"=\"*60,\n",
    "            \"\",\n",
    "            \"OVERALL METRICS:\",\n",
    "            f\"  Status: {self.overall_status}\",\n",
    "            f\"  MAE: {self.overall_mae:.4f}\",\n",
    "            f\"  Improvement: {self.overall_improvement:.1%}\",\n",
    "            f\"  DM p-value: {self.dm_pvalue:.4f}\" if self.dm_pvalue else \"  DM: N/A\",\n",
    "            f\"  MC-SS: {self.mc_ss:.3f} (reliable: {self.mc_reliable})\",\n",
    "            f\"  Conformal coverage: {self.conformal_coverage:.1%}\",\n",
    "            \"\",\n",
    "            \"PER-REGIME RESULTS:\",\n",
    "        ]\n",
    "        \n",
    "        for regime, results in sorted(self.regime_results.items()):\n",
    "            lines.append(f\"  [{regime}] (n={results.get('n', 0)}):\")\n",
    "            lines.append(f\"    MAE: {results.get('mae', 0):.4f}\")\n",
    "            lines.append(f\"    Improvement: {results.get('improvement', 0):.1%}\")\n",
    "            lines.append(f\"    Status: {results.get('status', 'N/A')}\")\n",
    "        \n",
    "        lines.extend([\n",
    "            \"\",\n",
    "            f\"WORST REGIME: {self.worst_regime}\",\n",
    "            \"=\"*60,\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "def complete_evaluation(\n",
    "    model,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    train_end_idx: int,\n",
    "    regimes: np.ndarray,\n",
    "    random_state: int = 42,\n",
    ") -> CompleteEvaluation:\n",
    "    \"\"\"\n",
    "    Run complete model evaluation with stratification.\n",
    "    \n",
    "    Combines:\n",
    "    - Shuffled target gate\n",
    "    - Suspicious improvement gate\n",
    "    - DM test\n",
    "    - MC-SS\n",
    "    - Conformal prediction\n",
    "    - Per-regime breakdown\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, y_train = X[:train_end_idx], y[:train_end_idx]\n",
    "    X_test, y_test = X[train_end_idx:], y[train_end_idx:]\n",
    "    regimes_test = regimes[train_end_idx:]\n",
    "    \n",
    "    # Get predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    persistence = X_test[:, -1]\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_mae = float(np.mean(np.abs(y_test - predictions)))\n",
    "    persistence_mae = float(np.mean(np.abs(np.diff(y_test))))\n",
    "    improvement = 1 - overall_mae / persistence_mae\n",
    "    \n",
    "    # Determine overall status\n",
    "    if improvement > 0.20:\n",
    "        overall_status = \"HALT (suspicious)\"\n",
    "    elif improvement > 0.10:\n",
    "        overall_status = \"WARN\"\n",
    "    elif improvement > 0:\n",
    "        overall_status = \"PASS\"\n",
    "    else:\n",
    "        overall_status = \"FAIL (worse than persistence)\"\n",
    "    \n",
    "    # DM test\n",
    "    try:\n",
    "        model_errors = y_test - predictions\n",
    "        persistence_errors = y_test - persistence\n",
    "        dm = dm_test(model_errors, persistence_errors)\n",
    "        dm_pvalue = dm.pvalue\n",
    "    except:\n",
    "        dm_pvalue = None\n",
    "    \n",
    "    # MC-SS\n",
    "    threshold = compute_move_threshold(np.diff(y_train))\n",
    "    mc = compute_move_conditional_metrics(\n",
    "        np.diff(predictions), np.diff(y_test), threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Conformal\n",
    "    _, quality = walk_forward_conformal(predictions, y_test, alpha=0.10)\n",
    "    \n",
    "    # Per-regime results\n",
    "    regime_results = {}\n",
    "    worst_regime = None\n",
    "    worst_improvement = float('inf')\n",
    "    \n",
    "    for regime in np.unique(regimes_test):\n",
    "        mask = regimes_test == regime\n",
    "        if mask.sum() < 10:\n",
    "            continue\n",
    "        \n",
    "        regime_actuals = y_test[mask]\n",
    "        regime_preds = predictions[mask]\n",
    "        \n",
    "        regime_mae = float(np.mean(np.abs(regime_actuals - regime_preds)))\n",
    "        regime_pers = float(np.mean(np.abs(np.diff(regime_actuals))))\n",
    "        regime_imp = 1 - regime_mae / regime_pers if regime_pers > 0 else 0\n",
    "        \n",
    "        if regime_imp > 0.20:\n",
    "            status = \"HALT\"\n",
    "        elif regime_imp > 0.10:\n",
    "            status = \"WARN\"\n",
    "        elif regime_imp > 0:\n",
    "            status = \"PASS\"\n",
    "        else:\n",
    "            status = \"FAIL\"\n",
    "        \n",
    "        regime_results[regime] = {\n",
    "            \"n\": int(mask.sum()),\n",
    "            \"mae\": regime_mae,\n",
    "            \"improvement\": regime_imp,\n",
    "            \"status\": status,\n",
    "        }\n",
    "        \n",
    "        if regime_imp < worst_improvement:\n",
    "            worst_improvement = regime_imp\n",
    "            worst_regime = regime\n",
    "    \n",
    "    return CompleteEvaluation(\n",
    "        overall_status=overall_status,\n",
    "        overall_mae=overall_mae,\n",
    "        overall_improvement=improvement,\n",
    "        dm_pvalue=dm_pvalue,\n",
    "        mc_ss=mc.skill_score,\n",
    "        mc_reliable=mc.is_reliable,\n",
    "        conformal_coverage=quality['coverage'],\n",
    "        regime_results=regime_results,\n",
    "        worst_regime=worst_regime or \"N/A\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete evaluation\n",
    "eval_result = complete_evaluation(\n",
    "    model=Ridge(alpha=1.0),\n",
    "    X=X,\n",
    "    y=y_target,\n",
    "    train_end_idx=train_size,\n",
    "    regimes=regimes,\n",
    ")\n",
    "\n",
    "print(eval_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pitfall Section: Common Mistakes\n",
    "\n",
    "### Pitfall 1: Regime from Full Series\n",
    "\n",
    "```python\n",
    "# WRONG: Compute regime boundaries from all data\n",
    "all_vol = compute_volatility(y_all)\n",
    "regime_labels = classify_regimes(all_vol)  # Leaks future!\n",
    "\n",
    "# RIGHT: Compute from training data only\n",
    "train_vol = compute_volatility(y_train)\n",
    "regime_thresholds = np.percentile(train_vol, [33, 67])\n",
    "regime_labels = classify_with_thresholds(test_vol, regime_thresholds)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 2: Aggregate-Only Validation\n",
    "\n",
    "```python\n",
    "# WRONG: Only check overall metrics\n",
    "if overall_improvement > 0:\n",
    "    deploy(model)  # May fail in critical regimes!\n",
    "\n",
    "# RIGHT: Check per-regime performance\n",
    "for regime, results in eval_result.regime_results.items():\n",
    "    if results['status'] == 'FAIL':\n",
    "        raise ValueError(f\"Model fails in {regime} regime\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 3: Ignoring Masked Regimes\n",
    "\n",
    "```python\n",
    "# WRONG: Ignore regimes with few samples\n",
    "if len(stratified_report.masked_regimes) > 0:\n",
    "    pass  # Ignore them\n",
    "\n",
    "# RIGHT: Acknowledge limitations\n",
    "if len(stratified_report.masked_regimes) > 0:\n",
    "    print(f\"Warning: Could not evaluate: {stratified_report.masked_regimes}\")\n",
    "    print(\"Consider collecting more data for these regimes\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "```\n",
    "â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "1. Aggregate metrics hide regime-specific failures\n",
    "   - Model may pass overall but fail in HIGH volatility\n",
    "   - Always stratify by regime\n",
    "\n",
    "2. run_gates_stratified() automates stratification\n",
    "   - regimes=\"auto\" detects volatility regimes\n",
    "   - min_n_per_regime masks unreliable regimes\n",
    "\n",
    "3. Complete evaluation combines all techniques\n",
    "   - Gates: shuffled_target, suspicious_improvement\n",
    "   - Tests: DM, PT\n",
    "   - Metrics: MC-SS, direction accuracy\n",
    "   - Intervals: conformal coverage\n",
    "\n",
    "4. Focus on the worst regime\n",
    "   - Model fails when it matters most\n",
    "   - HIGH volatility is often most important\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Curriculum Summary\n",
    "\n",
    "Congratulations! You've completed the full temporalcv curriculum.\n",
    "\n",
    "### Tier 1: Foundation (Why Time-Series is Different)\n",
    "| Notebook | Key Concept |\n",
    "|----------|-------------|\n",
    "| 01 | KFold leakage, WalkForwardCV |\n",
    "| 02 | Gap enforcement, horizon rule |\n",
    "| 03 | Persistence baseline, MASE |\n",
    "| 04 | Autocorrelation, HAC variance |\n",
    "\n",
    "### Tier 2: Prevention (Detecting and Avoiding Leakage)\n",
    "| Notebook | Key Concept |\n",
    "|----------|-------------|\n",
    "| 05 | Shuffled target gate, permutation testing |\n",
    "| 06 | Safe feature engineering |\n",
    "| 07 | Threshold and regime leakage |\n",
    "| 08 | HALT/WARN/PASS validation workflow |\n",
    "\n",
    "### Tier 3: Evaluation (High-Persistence and Advanced Metrics)\n",
    "| Notebook | Key Concept |\n",
    "|----------|-------------|\n",
    "| 09 | DM test, PT test, model comparison |\n",
    "| 10 | MC-SS, move-conditional metrics |\n",
    "| 11 | Conformal prediction, coverage guarantees |\n",
    "| **12** | **Regime-stratified evaluation (capstone)** |\n",
    "\n",
    "---\n",
    "\n",
    "### What You Can Now Do\n",
    "\n",
    "1. **Validate correctly**: Use walk-forward CV instead of KFold\n",
    "2. **Detect leakage**: Run shuffled target tests to catch subtle bugs\n",
    "3. **Enforce gaps**: Configure proper gaps for multi-step forecasting\n",
    "4. **Evaluate properly**: Use MC-SS and move-conditional metrics\n",
    "5. **Quantify uncertainty**: Build prediction intervals with guarantees\n",
    "6. **Stratify by regime**: Expose hidden failures in critical conditions\n",
    "\n",
    "---\n",
    "\n",
    "**Happy forecasting!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}