{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 09: Statistical Tests for Model Comparison\n\n**Diebold-Mariano, Pesaran-Timmermann, and HAC Variance**\n\n---\n\n## ðŸš¨ If You Know sklearn But Not Forecast Comparison, Read This First\n\n**What you already know (from standard ML)**:\n- Compare models by error: \"Model A has lower MAE than Model B\"\n- Use t-test: `ttest_rel(errors_A, errors_B)` to check significance\n- p < 0.05 means the difference is real\n- Done!\n\n**What's different with time series**:\n\n| Standard ML | Time Series Reality |\n|-------------|---------------------|\n| \"Model A: MAE=0.08, Model B: MAE=0.09\" | Sampling variability may reverse this |\n| `ttest_rel` for significance | **Invalid**: assumes independent errors |\n| Independent errors | Errors are **correlated** (MA(h-1) structure) |\n| Standard SE = Ïƒ/âˆšn | Underestimates variance â†’ false significance |\n\n**The problem in practice**:\n\n```python\n# You ran this:\nfrom scipy.stats import ttest_rel\nstat, p = ttest_rel(model_A_errors, model_B_errors)\n# p = 0.02 â†’ \"Model A is significantly better!\"\n\n# Reality:\n# Errors are autocorrelated â†’ SE is too small â†’ p is inflated\n# Proper test: p = 0.15 â†’ \"No significant difference\"\n```\n\n**The fix**: Use the Diebold-Mariano test with HAC variance.\n\n---\n\n## What You'll Learn\n\n1. **Why MAE comparison isn't enough** â€” Sampling variability and autocorrelation invalidate naive comparisons\n2. **Diebold-Mariano test** â€” Rigorous statistical test for comparing forecast accuracy [T1]\n3. **Pesaran-Timmermann test** â€” Testing directional accuracy significance [T1]\n\n**Prerequisites**: Notebooks 01-04 (Foundation tier)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: \"My Model Has Lower MAE\" Isn't Enough\n",
    "\n",
    "Consider this common scenario:\n",
    "\n",
    "```\n",
    "Model A MAE: 0.0823\n",
    "Model B MAE: 0.0891\n",
    "Conclusion: Model A is better!\n",
    "```\n",
    "\n",
    "**Why this reasoning fails:**\n",
    "\n",
    "1. **Sampling variability** â€” With different test data, Model B might win\n",
    "2. **Autocorrelated errors** â€” Standard t-tests assume independent errors (violated in time series)\n",
    "3. **Multiple comparisons** â€” Testing 5 models inflates false positive rate\n",
    "\n",
    "**The solution**: Statistical tests designed for forecast comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from temporalcv.statistical_tests import (\n",
    "    dm_test,\n",
    "    pt_test,\n",
    "    compare_multiple_models,\n",
    ")\n",
    "from temporalcv.cv import WalkForwardCV\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"temporalcv statistical tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "def generate_ar1(n: int, phi: float = 0.9, sigma: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"Generate AR(1) process: y[t] = phi * y[t-1] + epsilon[t]\"\"\"\n",
    "    y = np.zeros(n)\n",
    "    y[0] = np.random.normal(0, sigma)\n",
    "    for t in range(1, n):\n",
    "        y[t] = phi * y[t - 1] + np.random.normal(0, sigma)\n",
    "    return y\n",
    "\n",
    "# Create dataset\n",
    "n = 500\n",
    "y = generate_ar1(n, phi=0.9)\n",
    "\n",
    "# Create features (lagged values)\n",
    "X = np.column_stack([y[:-2], y[1:-1]])\n",
    "y_target = y[2:]\n",
    "\n",
    "# Train-test split (temporal)\n",
    "train_size = int(len(y_target) * 0.7)\n",
    "X_train, y_train = X[:train_size], y_target[:train_size]\n",
    "X_test, y_test = X[train_size:], y_target[train_size:]\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.01),\n",
    "    \"RF\": RandomForestRegressor(n_estimators=50, max_depth=3, random_state=42),\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "errors = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    predictions[name] = pred\n",
    "    errors[name] = y_test - pred  # Error = actual - prediction\n",
    "    mae = np.mean(np.abs(errors[name]))\n",
    "    print(f\"{name} MAE: {mae:.4f}\")\n",
    "\n",
    "# Add persistence baseline\n",
    "persistence_pred = X_test[:, -1]  # y[t-1] as prediction for y[t]\n",
    "predictions[\"Persistence\"] = persistence_pred\n",
    "errors[\"Persistence\"] = y_test - persistence_pred\n",
    "print(f\"Persistence MAE: {np.mean(np.abs(errors['Persistence'])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive conclusion**: The model with lowest MAE is \"best\".\n",
    "\n",
    "**But is this difference statistically significant?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Diebold-Mariano Test [T1]\n",
    "\n",
    "The DM test compares the predictive accuracy of two forecasts.\n",
    "\n",
    "**Null hypothesis**: Both forecasts have equal expected loss.\n",
    "\n",
    "**Key features:**\n",
    "- Handles autocorrelated forecast errors (via HAC variance)\n",
    "- Works with any loss function (squared, absolute)\n",
    "- Harvey correction for small samples [T1]\n",
    "\n",
    "**Citation**: Diebold & Mariano (1995), Harvey et al. (1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Ridge vs Persistence\n",
    "result = dm_test(\n",
    "    errors[\"Ridge\"],\n",
    "    errors[\"Persistence\"],\n",
    "    h=1,  # 1-step forecast\n",
    "    loss=\"squared\",  # MSE loss\n",
    "    harvey_correction=True,  # Small-sample adjustment\n",
    ")\n",
    "\n",
    "print(f\"DM statistic: {result.statistic:.3f}\")\n",
    "print(f\"P-value: {result.pvalue:.4f}\")\n",
    "print(f\"Mean loss differential: {result.mean_loss_diff:.6f}\")\n",
    "print(f\"Significant at 5%? {result.pvalue < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting DM Results\n",
    "\n",
    "- **Negative statistic**: Model 1 (Ridge) has lower loss â†’ Ridge is better\n",
    "- **Positive statistic**: Model 2 (Persistence) has lower loss â†’ Persistence is better\n",
    "- **p < 0.05**: Reject null hypothesis â€” difference is significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMTestResult has convenient properties\n",
    "print(f\"\\nResult summary:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAC Variance for h-step Forecasts\n",
    "\n",
    "For h-step ahead forecasts, errors follow an MA(h-1) structure:\n",
    "- 1-step: Errors are uncorrelated (standard variance)\n",
    "- 2-step: Errors have lag-1 autocorrelation\n",
    "- h-step: Errors have autocorrelation up to lag h-1\n",
    "\n",
    "**HAC (Heteroskedasticity and Autocorrelation Consistent) variance** accounts for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 1-step vs 2-step horizon\n",
    "print(\"Effect of horizon on DM test:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for h in [1, 2, 3]:\n",
    "    result = dm_test(\n",
    "        errors[\"Ridge\"],\n",
    "        errors[\"Persistence\"],\n",
    "        h=h,\n",
    "        loss=\"squared\",\n",
    "    )\n",
    "    print(f\"h={h}: DM={result.statistic:6.3f}, p={result.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: Higher horizon â†’ wider confidence intervals â†’ harder to reject null.\n",
    "\n",
    "This is correct behavior! More uncertainty in h-step forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pesaran-Timmermann Test [T1]\n",
    "\n",
    "The PT test evaluates **directional accuracy** â€” did you predict the right sign?\n",
    "\n",
    "**Null hypothesis**: Forecasts have no directional accuracy (random guessing).\n",
    "\n",
    "**Key features:**\n",
    "- Compares observed accuracy to expected under independence\n",
    "- Accounts for marginal distributions (not just 50%)\n",
    "- Works on changes/returns (not levels)\n",
    "\n",
    "**Citation**: Pesaran & Timmermann (1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute changes (PT test works on changes, not levels)\n",
    "actual_changes = np.diff(y_test)\n",
    "ridge_changes = np.diff(predictions[\"Ridge\"])\n",
    "\n",
    "# Run PT test\n",
    "result = pt_test(actual_changes, ridge_changes)\n",
    "\n",
    "print(f\"Observed accuracy: {result.accuracy:.1%}\")\n",
    "print(f\"Expected accuracy: {result.expected:.1%}\")\n",
    "print(f\"PT statistic: {result.statistic:.3f}\")\n",
    "print(f\"P-value: {result.pvalue:.4f}\")\n",
    "print(f\"Significant? {result.pvalue < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Expected Accuracy â‰  50%\n",
    "\n",
    "The expected accuracy under independence depends on marginal distributions:\n",
    "\n",
    "```\n",
    "Expected = P(actual > 0) Ã— P(pred > 0) + P(actual < 0) Ã— P(pred < 0)\n",
    "```\n",
    "\n",
    "If actuals are 60% positive and predictions are 70% positive:\n",
    "- Expected = 0.6 Ã— 0.7 + 0.4 Ã— 0.3 = 0.54 (not 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test persistence baseline direction accuracy\n",
    "persistence_changes = np.diff(predictions[\"Persistence\"])\n",
    "\n",
    "result_persistence = pt_test(actual_changes, persistence_changes)\n",
    "print(f\"\\nPersistence direction accuracy:\")\n",
    "print(f\"Observed: {result_persistence.accuracy:.1%}\")\n",
    "print(f\"Expected: {result_persistence.expected:.1%}\")\n",
    "print(f\"P-value: {result_persistence.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Class PT Test [T3]\n",
    "\n",
    "The standard PT test uses 2 classes (UP/DOWN). With a threshold, we can use 3 classes:\n",
    "\n",
    "- **UP**: change > threshold\n",
    "- **DOWN**: change < -threshold\n",
    "- **FLAT**: |change| â‰¤ threshold\n",
    "\n",
    "**Note**: 3-class mode is [T3] â€” an exploratory extension, not academically validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-class PT test with move threshold\n",
    "move_threshold = np.percentile(np.abs(actual_changes), 70)\n",
    "\n",
    "result_3class = pt_test(\n",
    "    actual_changes, \n",
    "    ridge_changes,\n",
    "    move_threshold=move_threshold  # Enables 3-class mode\n",
    ")\n",
    "\n",
    "print(f\"3-class PT test (threshold={move_threshold:.4f}):\")\n",
    "print(f\"Observed accuracy: {result_3class.accuracy:.1%}\")\n",
    "print(f\"Expected accuracy: {result_3class.expected:.1%}\")\n",
    "print(f\"P-value: {result_3class.pvalue:.4f}\")\n",
    "print(f\"Number of classes: {result_3class.n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Multi-Model Comparison\n",
    "\n",
    "When comparing multiple models, we need:\n",
    "- All pairwise DM tests\n",
    "- **Bonferroni correction** for multiple comparisons\n",
    "\n",
    "**Without correction**: 5 models â†’ 10 pairs â†’ 40% false positive rate at Î±=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison = compare_multiple_models(\n",
    "    errors,  # Dict of model_name -> error_array\n",
    "    h=1,\n",
    "    alpha=0.05,\n",
    "    loss=\"squared\",\n",
    ")\n",
    "\n",
    "print(comparison.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific pairwise results\n",
    "print(f\"\\nBest model: {comparison.best_model}\")\n",
    "print(f\"\\nBonferroni-corrected alpha: {comparison.bonferroni_alpha:.4f}\")\n",
    "print(f\"\\nSignificant pairs (after correction): {comparison.significant_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific pairwise result\n",
    "ridge_vs_lasso = comparison.get_pairwise(\"Ridge\", \"Lasso\")\n",
    "if ridge_vs_lasso:\n",
    "    print(f\"Ridge vs Lasso:\")\n",
    "    print(f\"  DM statistic: {ridge_vs_lasso.statistic:.3f}\")\n",
    "    print(f\"  P-value: {ridge_vs_lasso.pvalue:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pitfall Section: Common Mistakes\n",
    "\n",
    "### Pitfall 1: Wrong Horizon Parameter\n",
    "\n",
    "```python\n",
    "# WRONG: Using h=1 for 3-step ahead forecasts\n",
    "result = dm_test(errors_3step, baseline_errors, h=1)  # Underestimates variance!\n",
    "\n",
    "# RIGHT: Match h to actual forecast horizon\n",
    "result = dm_test(errors_3step, baseline_errors, h=3)  # Correct HAC bandwidth\n",
    "```\n",
    "\n",
    "**Why it matters**: Wrong h â†’ incorrect variance estimate â†’ invalid p-values.\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 2: Ignoring Harvey Correction\n",
    "\n",
    "```python\n",
    "# WRONG: Disable Harvey correction with small samples\n",
    "result = dm_test(errors_a, errors_b, harvey_correction=False)  # n < 100\n",
    "\n",
    "# RIGHT: Always use Harvey correction (default)\n",
    "result = dm_test(errors_a, errors_b, harvey_correction=True)\n",
    "```\n",
    "\n",
    "**Why it matters**: Without correction, DM test has size distortions in small samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Pitfall 3: Too Few Observations\n",
    "\n",
    "```python\n",
    "# WRONG: Run DM test with n < 30\n",
    "result = dm_test(errors[:20], baseline[:20])  # ValueError!\n",
    "\n",
    "# RIGHT: Ensure n >= 30\n",
    "if len(errors) >= 30:\n",
    "    result = dm_test(errors, baseline)\n",
    "```\n",
    "\n",
    "**Why it matters**: Asymptotic approximation breaks down with few observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate minimum sample size requirement\n",
    "try:\n",
    "    small_errors = errors[\"Ridge\"][:20]\n",
    "    small_baseline = errors[\"Persistence\"][:20]\n",
    "    dm_test(small_errors, small_baseline)\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "```\n",
    "â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "1. MAE comparison alone is insufficient\n",
    "   - Use DM test for statistical significance\n",
    "   - Sampling variability can fool you\n",
    "\n",
    "2. DM test accounts for autocorrelated errors\n",
    "   - HAC variance for h-step forecasts\n",
    "   - Harvey correction for small samples [T1]\n",
    "\n",
    "3. PT test evaluates directional accuracy\n",
    "   - Expected accuracy â‰  50% (depends on marginals)\n",
    "   - 3-class mode is [T3] exploratory\n",
    "\n",
    "4. Multiple comparisons need correction\n",
    "   - Bonferroni: Î±_corrected = Î± / n_comparisons\n",
    "   - 5 models â†’ 10 pairs â†’ significant inflation\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "| Notebook | Topic |\n",
    "|----------|-------|\n",
    "| **10** | High-persistence metrics (MC-SS, Theil's U) |\n",
    "| 11 | Conformal prediction for uncertainty quantification |\n",
    "| 12 | Regime-stratified evaluation (capstone) |\n",
    "\n",
    "---\n",
    "\n",
    "**You've learned**: How to rigorously compare forecasting models using DM and PT tests with proper variance estimation and multiple comparison corrections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}