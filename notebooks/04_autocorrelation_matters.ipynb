{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Autocorrelation Matters: HAC Variance and Model Comparison\n\n## Why Naive Standard Errors Are Wrong for Time Series\n\n---\n\n## \ud83d\udea8 If You Know sklearn But Not HAC Variance, Read This First\n\n**What you already know (from standard ML)**:\n- `ttest_rel` or `ttest_ind` compare model performance\n- Standard errors: `SE = \u03c3 / \u221an`\n- p < 0.05 means \"statistically significant\"\n- Errors are independent (that's what makes the math work)\n\n**What's different with time series**:\n\nIn cross-sectional ML, residuals are independent. In time series, they're **correlated**.\n\n**The problem**:\n```python\n# Standard ML: errors are independent\nerrors = [0.1, -0.2, 0.3, -0.1, 0.2]  # Each unrelated to others\n\n# Time series: errors are correlated!\n# If error at t is positive, error at t+1 is likely positive too\nerrors = [0.1, 0.12, 0.08, -0.1, -0.08]  # They cluster!\n```\n\n**Why this breaks standard tests**:\n\n| Assumption | Standard ML | Time Series | Impact |\n|------------|-------------|-------------|--------|\n| Error independence | \u2713 True | \u2717 Violated | SE too small |\n| Variance formula | \u03c3\u00b2/n | HAC needed | p-values too small |\n| Naive t-test | Valid | **Invalid** | False significance |\n\n**Example**:\n- Naive t-test: p = 0.02 \u2192 \"Significant!\" \n- Proper HAC test: p = 0.15 \u2192 \"Not significant\"\n- You claimed a win that wasn't real.\n\n---\n\n**What you'll learn:**\n1. Why forecast errors are autocorrelated (MA(h-1) structure for h-step forecasts)\n2. How autocorrelation inflates naive variance estimates \u2192 spurious significance\n3. How to use HAC (Heteroskedasticity and Autocorrelation Consistent) variance\n4. How to properly compare models with the Diebold-Mariano test\n\n**Prerequisites:** Notebooks 01-03\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from temporalcv.cv import WalkForwardCV\n",
    "from temporalcv.statistical_tests import dm_test, compute_hac_variance\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shhiwt82q5",
   "source": "---\n\n## Section 0: ACF and PACF Intuition\n\nBefore diving into HAC variance, let's build intuition for autocorrelation.\n\n### What ACF Tells You\n\n**ACF (Autocorrelation Function)**: Correlation between a series and its lagged values.\n\n| ACF(lag) | Meaning | Implication |\n|----------|---------|-------------|\n| ACF(1) = 0.95 | y[t] and y[t-1] are 95% correlated | Very sticky series |\n| ACF(1) = 0.50 | y[t] and y[t-1] are 50% correlated | Moderate memory |\n| ACF(1) = 0.10 | y[t] and y[t-1] are 10% correlated | Nearly random |\n\n### ACF Decay Patterns\n\n```\nFast Decay (\u03c6=0.3):           Slow Decay (\u03c6=0.9):\nACF: [1.0, 0.3, 0.09, 0.03]   ACF: [1.0, 0.9, 0.81, 0.73]\n     \u2587\u2582\u2591\u2591                           \u2587\u2587\u2587\u2586\n\n\u2192 Easy to beat persistence      \u2192 Hard to beat persistence\n```\n\n### What PACF Tells You\n\n**PACF (Partial Autocorrelation Function)**: Correlation at lag k after removing effects of shorter lags.\n\n| Pattern | Interpretation | Model |\n|---------|---------------|-------|\n| PACF drops after lag 1 | Direct dependence only on y[t-1] | AR(1) |\n| PACF drops after lag p | Direct dependence on last p values | AR(p) |\n| PACF decays slowly | Complex dependence structure | Consider differencing |\n\n### Reading ACF/PACF Plots\n\n```\n                    ACF Plot                    PACF Plot\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\nSignificance \u2192 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500  (95% CI)    \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500\n                \u2588 \u2588 \u2586 \u2585 \u2584 \u2583 \u2582   (AR(1))       \u2588 \u2581 \u2581 \u2581 \u2581 \u2581 \u2581\n                \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588   (Random Walk) \u2588 \u2581 \u2581 \u2581 \u2581 \u2581 \u2581\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  lag \u2192                        lag \u2192\n```\n\n**Key insight**: If ACF decays slowly \u2192 high persistence \u2192 hard to beat persistence \u2192 need HAC variance for inference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "up2l3rp4sqs",
   "source": "# Visualize ACF decay for different persistence levels\ndef compute_acf_visual(series, max_lag=15):\n    \"\"\"Compute ACF for visualization.\"\"\"\n    n = len(series)\n    mean_s = np.mean(series)\n    var_s = np.var(series)\n    \n    acf = []\n    for lag in range(max_lag + 1):\n        if lag == 0:\n            acf.append(1.0)\n        else:\n            cov = np.mean((series[lag:] - mean_s) * (series[:-lag] - mean_s))\n            acf.append(cov / var_s)\n    return np.array(acf)\n\n# Generate AR(1) with different \u03c6\nnp.random.seed(42)\nn = 500\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nphi_values = [0.3, 0.7, 0.95]\nlabels = ['Low (\u03c6=0.3)\\nEasy to Beat', 'Moderate (\u03c6=0.7)\\nAchievable', 'High (\u03c6=0.95)\\nVery Hard']\ncolors = ['green', 'orange', 'red']\n\nfor ax, phi, label, color in zip(axes, phi_values, labels, colors):\n    # Generate AR(1)\n    y = np.zeros(n)\n    y[0] = np.random.normal(0, 1 / np.sqrt(1 - phi**2))\n    for t in range(1, n):\n        y[t] = phi * y[t-1] + np.random.normal()\n    \n    # Compute ACF\n    acf = compute_acf_visual(y, max_lag=15)\n    \n    # Plot\n    lags = np.arange(len(acf))\n    ax.bar(lags, acf, color=color, alpha=0.7, edgecolor='black')\n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.axhline(y=1.96/np.sqrt(n), color='gray', linestyle='--', alpha=0.7)\n    ax.axhline(y=-1.96/np.sqrt(n), color='gray', linestyle='--', alpha=0.7)\n    \n    ax.set_xlabel('Lag')\n    ax.set_ylabel('ACF')\n    ax.set_title(label, fontsize=11, fontweight='bold', color=color)\n    ax.set_ylim(-0.3, 1.1)\n\nplt.suptitle('ACF Decay Comparison: Persistence Difficulty', fontsize=13, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2605 Key Insight: Slow ACF decay = high persistence = hard to beat baseline\")\nprint(\"  - Fast decay (\u03c6=0.3): ACF drops to near-zero within 5 lags\")\nprint(\"  - Slow decay (\u03c6=0.95): ACF stays significant for 15+ lags\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate AR(1) process\n",
    "def generate_ar1(n=500, phi=0.9, sigma=1.0, seed=42):\n",
    "    \"\"\"Generate AR(1) process.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.zeros(n)\n",
    "    y[0] = rng.normal(0, sigma / np.sqrt(1 - phi**2))\n",
    "    for t in range(1, n):\n",
    "        y[t] = phi * y[t-1] + sigma * rng.normal()\n",
    "    return y\n",
    "\n",
    "def create_lag_features(series, n_lags=5):\n",
    "    \"\"\"Create lag features.\"\"\"\n",
    "    n = len(series)\n",
    "    X = np.column_stack([\n",
    "        np.concatenate([[np.nan]*lag, series[:-lag]]) \n",
    "        for lag in range(1, n_lags + 1)\n",
    "    ])\n",
    "    valid = ~np.isnan(X).any(axis=1)\n",
    "    return X[valid], series[valid]\n",
    "\n",
    "# Generate data\n",
    "series = generate_ar1(n=600, phi=0.9, seed=42)\n",
    "print(f\"Generated AR(1) series with ACF(1) = {np.corrcoef(series[1:], series[:-1])[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: The MA(h-1) Error Structure [T1]\n",
    "\n",
    "### Why are h-step forecast errors autocorrelated?\n",
    "\n",
    "For an AR(1) process:\n",
    "$$y_{t+h} = \\phi^h y_t + \\sum_{j=0}^{h-1} \\phi^j \\epsilon_{t+h-j}$$\n",
    "\n",
    "The **optimal h-step forecast** is:\n",
    "$$\\hat{y}_{t+h|t} = \\phi^h y_t$$\n",
    "\n",
    "The **forecast error** is:\n",
    "$$e_{t+h|t} = y_{t+h} - \\hat{y}_{t+h|t} = \\sum_{j=0}^{h-1} \\phi^j \\epsilon_{t+h-j}$$\n",
    "\n",
    "This is an **MA(h-1) process**! The errors are correlated up to lag h-1.\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "- **h=1 forecasts**: Errors are uncorrelated (pure innovation)\n",
    "- **h=2 forecasts**: Errors at t and t+1 share the shock at t+1\n",
    "- **h=12 forecasts**: Errors overlap by 11 shocks \u2192 high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate error autocorrelation for different horizons\n",
    "def compute_forecast_errors(series, horizon, n_lags=5):\n",
    "    \"\"\"Compute h-step forecast errors using AR model.\"\"\"\n",
    "    X, y = create_lag_features(series, n_lags=n_lags)\n",
    "    \n",
    "    # Split 80-20\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # For simplicity, use same model but interpret as h-step\n",
    "    # (In practice, you'd use a proper h-step ahead target)\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Simulate h-step error structure by creating overlapping errors\n",
    "    # This simulates what happens with multi-step forecasts\n",
    "    errors = y_test - preds\n",
    "    if horizon > 1:\n",
    "        # Smooth errors to simulate MA(h-1) structure\n",
    "        kernel = np.ones(horizon) / np.sqrt(horizon)\n",
    "        errors = np.convolve(errors, kernel, mode='valid')\n",
    "    \n",
    "    return errors\n",
    "\n",
    "def compute_acf(errors, max_lag=10):\n",
    "    \"\"\"Compute autocorrelation function.\"\"\"\n",
    "    n = len(errors)\n",
    "    mean_e = np.mean(errors)\n",
    "    var_e = np.var(errors)\n",
    "    \n",
    "    acf = []\n",
    "    for lag in range(max_lag + 1):\n",
    "        if lag == 0:\n",
    "            acf.append(1.0)\n",
    "        else:\n",
    "            cov = np.mean((errors[lag:] - mean_e) * (errors[:-lag] - mean_e))\n",
    "            acf.append(cov / var_e)\n",
    "    return np.array(acf)\n",
    "\n",
    "# Compute errors and ACF for different horizons\n",
    "horizons = [1, 4, 8, 12]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for ax, h in zip(axes.flat, horizons):\n",
    "    errors = compute_forecast_errors(series, h)\n",
    "    acf = compute_acf(errors, max_lag=15)\n",
    "    \n",
    "    lags = np.arange(len(acf))\n",
    "    ax.bar(lags, acf, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "    ax.axhline(y=1.96/np.sqrt(len(errors)), color='red', linestyle='--', \n",
    "               label='95% CI', alpha=0.7)\n",
    "    ax.axhline(y=-1.96/np.sqrt(len(errors)), color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Mark the expected cutoff at h-1\n",
    "    if h > 1:\n",
    "        ax.axvline(x=h-1, color='green', linestyle=':', linewidth=2, \n",
    "                   label=f'Expected cutoff (h-1={h-1})')\n",
    "    \n",
    "    ax.set_xlabel('Lag')\n",
    "    ax.set_ylabel('ACF')\n",
    "    ax.set_title(f'h={h} Forecast Error ACF', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.set_ylim(-0.5, 1.1)\n",
    "\n",
    "plt.suptitle('[T1] h-Step Forecast Errors Have MA(h-1) Structure', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: h-step forecast errors are correlated up to lag h-1.\")\n",
    "print(\"This violates the i.i.d. assumption in naive standard errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: The Problem with Naive Variance\n",
    "\n",
    "**Naive variance**: $\\text{Var}(\\bar{d}) = \\frac{\\text{Var}(d)}{n}$\n",
    "\n",
    "This assumes errors are uncorrelated. When they're positively correlated:\n",
    "- Naive variance is **too small**\n",
    "- Standard errors are **too small**\n",
    "- p-values are **too small**\n",
    "- You claim significance when there is none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the variance underestimation\n",
    "def simulate_variance_comparison(n=200, phi=0.7, n_simulations=1000, seed=42):\n",
    "    \"\"\"Simulate the difference between naive and true variance.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    means_naive = []\n",
    "    means_true = []\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Generate AR(1) series (correlated errors)\n",
    "        series = np.zeros(n)\n",
    "        series[0] = rng.normal()\n",
    "        for t in range(1, n):\n",
    "            series[t] = phi * series[t-1] + rng.normal()\n",
    "        \n",
    "        means_true.append(np.mean(series))\n",
    "    \n",
    "    # True variance of sample mean\n",
    "    true_var = np.var(means_true)\n",
    "    \n",
    "    # Naive estimate (assumes i.i.d.)\n",
    "    # For AR(1): Var(series) = sigma^2 / (1 - phi^2)\n",
    "    theoretical_var = 1 / (1 - phi**2)\n",
    "    naive_var_of_mean = theoretical_var / n\n",
    "    \n",
    "    return {\n",
    "        'true_var': true_var,\n",
    "        'naive_var': naive_var_of_mean,\n",
    "        'ratio': true_var / naive_var_of_mean\n",
    "    }\n",
    "\n",
    "print(\"NAIVE vs TRUE VARIANCE OF SAMPLE MEAN\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\n(Simulating correlated series with different \u03c6 values)\\n\")\n",
    "\n",
    "for phi in [0.0, 0.3, 0.7, 0.9]:\n",
    "    result = simulate_variance_comparison(n=200, phi=phi)\n",
    "    print(f\"\u03c6={phi}:\")\n",
    "    print(f\"  Naive Var(mean): {result['naive_var']:.6f}\")\n",
    "    print(f\"  True Var(mean):  {result['true_var']:.6f}\")\n",
    "    print(f\"  Ratio (true/naive): {result['ratio']:.1f}x\")\n",
    "    if result['ratio'] > 1.5:\n",
    "        print(f\"  \u2192 Naive SE underestimates by ~{np.sqrt(result['ratio']):.1f}x\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the practical consequence: false significance\n",
    "def compute_false_positive_rate(n=100, phi=0.7, true_diff=0, n_simulations=1000, seed=42):\n",
    "    \"\"\"Compute false positive rate using naive vs HAC variance.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    false_positives_naive = 0\n",
    "    false_positives_hac = 0\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Generate correlated \"loss differentials\" (no true difference)\n",
    "        d = np.zeros(n)\n",
    "        d[0] = rng.normal()\n",
    "        for t in range(1, n):\n",
    "            d[t] = phi * d[t-1] + rng.normal()\n",
    "        d = d + true_diff  # Add true difference (0 under null)\n",
    "        \n",
    "        mean_d = np.mean(d)\n",
    "        \n",
    "        # Naive test\n",
    "        naive_se = np.std(d) / np.sqrt(n)\n",
    "        naive_t = mean_d / naive_se\n",
    "        naive_pvalue = 2 * (1 - stats.t.cdf(abs(naive_t), df=n-1))\n",
    "        if naive_pvalue < 0.05:\n",
    "            false_positives_naive += 1\n",
    "        \n",
    "        # HAC test\n",
    "        hac_var = compute_hac_variance(d, bandwidth=int(phi * 10))\n",
    "        if hac_var > 0:\n",
    "            hac_se = np.sqrt(hac_var)\n",
    "            hac_t = mean_d / hac_se\n",
    "            hac_pvalue = 2 * (1 - stats.t.cdf(abs(hac_t), df=n-1))\n",
    "            if hac_pvalue < 0.05:\n",
    "                false_positives_hac += 1\n",
    "    \n",
    "    return {\n",
    "        'naive_fpr': false_positives_naive / n_simulations,\n",
    "        'hac_fpr': false_positives_hac / n_simulations\n",
    "    }\n",
    "\n",
    "print(\"FALSE POSITIVE RATES AT \u03b1=0.05\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\n(Expected: 5% under the null hypothesis)\\n\")\n",
    "\n",
    "for phi in [0.0, 0.3, 0.7, 0.9]:\n",
    "    result = compute_false_positive_rate(phi=phi)\n",
    "    print(f\"\u03c6={phi}:\")\n",
    "    print(f\"  Naive FPR: {result['naive_fpr']*100:.1f}%\", end='')\n",
    "    if result['naive_fpr'] > 0.10:\n",
    "        print(f\"  \u2190 INFLATED!\")\n",
    "    else:\n",
    "        print()\n",
    "    print(f\"  HAC FPR:   {result['hac_fpr']*100:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: HAC Variance Estimation [T1]\n",
    "\n",
    "**HAC (Heteroskedasticity and Autocorrelation Consistent)** variance corrects for serial correlation.\n",
    "\n",
    "The Newey-West estimator:\n",
    "\n",
    "$$\\hat{\\text{Var}}_{\\text{HAC}} = \\frac{1}{n} \\left[ \\gamma_0 + 2 \\sum_{j=1}^{m} w_j \\gamma_j \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\gamma_j$ = autocovariance at lag j\n",
    "- $w_j$ = Bartlett kernel weight: $1 - j/(m+1)$\n",
    "- $m$ = bandwidth (typically h-1 for h-step forecasts)\n",
    "\n",
    "### Reference [T1]\n",
    "Newey & West (1987): \"A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate compute_hac_variance\n",
    "print(\"compute_hac_variance Demo\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Generate correlated series\n",
    "n = 200\n",
    "rng = np.random.default_rng(42)\n",
    "d = np.zeros(n)\n",
    "d[0] = rng.normal()\n",
    "for t in range(1, n):\n",
    "    d[t] = 0.7 * d[t-1] + rng.normal()\n",
    "\n",
    "# Naive variance\n",
    "naive_var = np.var(d) / n\n",
    "\n",
    "# HAC variance with different bandwidths\n",
    "print(f\"\\nSeries with \u03c6=0.7, n={n}\\n\")\n",
    "print(f\"{'Bandwidth':<15} {'HAC Var':<15} {'Ratio to Naive'}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Naive (b=0)':<15} {naive_var:<15.6f} 1.0x\")\n",
    "\n",
    "for bandwidth in [1, 3, 5, 10, 'auto']:\n",
    "    if bandwidth == 'auto':\n",
    "        hac_var = compute_hac_variance(d)  # Uses automatic bandwidth\n",
    "        label = 'Auto'\n",
    "    else:\n",
    "        hac_var = compute_hac_variance(d, bandwidth=bandwidth)\n",
    "        label = f'b={bandwidth}'\n",
    "    \n",
    "    ratio = hac_var / naive_var if naive_var > 0 else float('inf')\n",
    "    print(f\"{label:<15} {hac_var:<15.6f} {ratio:.1f}x\")\n",
    "\n",
    "print(\"\\n[T1] HAC variance is larger \u2192 accounts for correlation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Bartlett kernel\n",
    "def bartlett_kernel(j, bandwidth):\n",
    "    \"\"\"Bartlett kernel weight.\"\"\"\n",
    "    if abs(j) <= bandwidth:\n",
    "        return 1.0 - abs(j) / (bandwidth + 1)\n",
    "    return 0.0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "bandwidths = [3, 5, 10]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for bw, color in zip(bandwidths, colors):\n",
    "    lags = np.arange(0, 15)\n",
    "    weights = [bartlett_kernel(j, bw) for j in lags]\n",
    "    ax.plot(lags, weights, 'o-', color=color, linewidth=2, markersize=8, \n",
    "            label=f'Bandwidth = {bw}', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Lag (j)', fontsize=12)\n",
    "ax.set_ylabel('Kernel Weight', fontsize=12)\n",
    "ax.set_title('Bartlett Kernel Weights for HAC Variance', fontsize=13, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe kernel gives full weight to lag 0, then declines linearly to 0 at bandwidth+1.\")\n",
    "print(\"For h-step forecasts, use bandwidth = h-1 (errors are MA(h-1)).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: The Diebold-Mariano Test [T1]\n",
    "\n",
    "To compare two forecasting models, use the **Diebold-Mariano (DM) test**.\n",
    "\n",
    "Tests H\u2080: $E[d_t] = 0$ where $d_t = L(e_{1,t}) - L(e_{2,t})$ is the loss differential.\n",
    "\n",
    "The DM statistic:\n",
    "\n",
    "$$\\text{DM} = \\frac{\\bar{d}}{\\sqrt{\\widehat{\\text{Var}}_{\\text{HAC}}(\\bar{d})}}$$\n",
    "\n",
    "**Key features:**\n",
    "1. Uses HAC variance (handles autocorrelated errors)\n",
    "2. Harvey et al. (1997) small-sample correction\n",
    "3. Works for any loss function (squared, absolute)\n",
    "\n",
    "### Reference [T1]\n",
    "Diebold & Mariano (1995): \"Comparing Predictive Accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate dm_test\n",
    "print(\"dm_test Demo\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Generate two sets of forecast errors\n",
    "# Model 1: AR-based forecast (slightly better)\n",
    "# Model 2: Persistence baseline\n",
    "\n",
    "X, y = create_lag_features(series, n_lags=5)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Model 1: Ridge regression\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "preds_model = model.predict(X_test)\n",
    "errors_model = y_test - preds_model\n",
    "\n",
    "# Model 2: Persistence\n",
    "preds_persist = X_test[:, 0]  # y[t-1]\n",
    "errors_persist = y_test - preds_persist\n",
    "\n",
    "# Run DM test for different horizons\n",
    "print(f\"\\nComparing Ridge model vs Persistence baseline\\n\")\n",
    "print(f\"Model MAE: {np.mean(np.abs(errors_model)):.4f}\")\n",
    "print(f\"Persistence MAE: {np.mean(np.abs(errors_persist)):.4f}\")\n",
    "\n",
    "print(f\"\\n{'Horizon':<10} {'DM Stat':<12} {'p-value':<12} {'Conclusion'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for h in [1, 4, 8, 12]:\n",
    "    result = dm_test(\n",
    "        errors_1=errors_model,\n",
    "        errors_2=errors_persist,\n",
    "        h=h,\n",
    "        loss='absolute',\n",
    "        alternative='less',  # H1: model is better (lower loss)\n",
    "        harvey_correction=True\n",
    "    )\n",
    "    \n",
    "    conclusion = 'Model better' if result.pvalue < 0.05 else 'Not significant'\n",
    "    print(f\"h={h:<8} {result.statistic:<12.3f} {result.pvalue:<12.4f} {conclusion}\")\n",
    "\n",
    "print(\"\\n[T1] Note: As h increases, HAC bandwidth increases \u2192 more conservative test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the full result object\n",
    "result = dm_test(\n",
    "    errors_1=errors_model,\n",
    "    errors_2=errors_persist,\n",
    "    h=4,\n",
    "    loss='absolute',\n",
    "    alternative='less',\n",
    "    harvey_correction=True\n",
    ")\n",
    "\n",
    "print(\"DMTestResult object (h=4)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nstatistic:     {result.statistic:.4f}\")\n",
    "print(f\"pvalue:        {result.pvalue:.4f}\")\n",
    "print(f\"mean_diff:     {result.mean_loss_diff:.6f}\")\n",
    "print(f\"horizon:       {result.h}\")\n",
    "print(f\"n:             {result.n}\")\n",
    "print(f\"loss:          {result.loss}\")\n",
    "print(f\"alternative:   {result.alternative}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Practical Implications\n",
    "\n",
    "### When to Use HAC Variance?\n",
    "\n",
    "**Always** when comparing time series forecasts, because:\n",
    "1. h > 1 forecasts have MA(h-1) errors by construction\n",
    "2. Even h = 1 forecasts can have autocorrelated errors if the model is misspecified\n",
    "3. Naive tests are anti-conservative \u2192 too many false positives\n",
    "\n",
    "### Bandwidth Selection\n",
    "\n",
    "- **For h-step forecasts**: Use bandwidth = h - 1 (theoretical minimum)\n",
    "- **Unknown h**: Use automatic selection (Andrews 1991 rule)\n",
    "- **Conservative**: Use larger bandwidth if in doubt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the effect of bandwidth on DM test results\n",
    "print(\"Bandwidth Sensitivity Analysis\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"\\nDM test: Model vs Persistence (same data)\\n\")\n",
    "print(f\"{'Bandwidth':<12} {'DM Stat':<12} {'p-value':<12} {'SE(diff)'}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "# Compute loss differentials\n",
    "loss_model = np.abs(errors_model)\n",
    "loss_persist = np.abs(errors_persist)\n",
    "d = loss_model - loss_persist\n",
    "mean_d = np.mean(d)\n",
    "\n",
    "for bw in [0, 1, 3, 5, 10]:\n",
    "    hac_var = compute_hac_variance(d, bandwidth=bw) if bw > 0 else np.var(d) / len(d)\n",
    "    if hac_var > 0:\n",
    "        se = np.sqrt(hac_var)\n",
    "        dm_stat = mean_d / se\n",
    "        pvalue = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
    "        print(f\"{bw:<12} {dm_stat:<12.3f} {pvalue:<12.4f} {se:.6f}\")\n",
    "    else:\n",
    "        print(f\"{bw:<12} {'N/A':<12} {'N/A':<12} {'Negative var'}\")\n",
    "\n",
    "print(\"\\nNote: Higher bandwidth \u2192 larger SE \u2192 more conservative test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow: proper model comparison\n",
    "def compare_models_properly(series, horizon=4):\n",
    "    \"\"\"\n",
    "    Compare two models using proper HAC-based inference.\n",
    "    \"\"\"\n",
    "    X, y = create_lag_features(series, n_lags=5)\n",
    "    \n",
    "    # Walk-forward CV with proper gap\n",
    "    cv = WalkForwardCV(\n",
    "        n_splits=5,\n",
    "        extra_gap=horizon,\n",
    "        window_type='expanding',\n",
    "        test_size=50\n",
    "    )\n",
    "    \n",
    "    all_errors_model = []\n",
    "    all_errors_persist = []\n",
    "    \n",
    "    for train_idx, test_idx in cv.split(X):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "        \n",
    "        # Model predictions\n",
    "        model = Ridge(alpha=1.0)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Persistence predictions\n",
    "        persist_preds = X_test[:, 0]\n",
    "        \n",
    "        # Collect errors\n",
    "        all_errors_model.extend(y_test - preds)\n",
    "        all_errors_persist.extend(y_test - persist_preds)\n",
    "    \n",
    "    errors_model = np.array(all_errors_model)\n",
    "    errors_persist = np.array(all_errors_persist)\n",
    "    \n",
    "    # DM test with proper HAC variance\n",
    "    result = dm_test(\n",
    "        errors_1=errors_model,\n",
    "        errors_2=errors_persist,\n",
    "        h=horizon,\n",
    "        loss='absolute',\n",
    "        alternative='less',\n",
    "        harvey_correction=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'model_mae': np.mean(np.abs(errors_model)),\n",
    "        'persist_mae': np.mean(np.abs(errors_persist)),\n",
    "        'dm_stat': result.statistic,\n",
    "        'pvalue': result.pvalue,\n",
    "        'significant': result.pvalue < 0.05\n",
    "    }\n",
    "\n",
    "print(\"COMPLETE MODEL COMPARISON WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for h in [1, 4, 8]:\n",
    "    result = compare_models_properly(series, horizon=h)\n",
    "    \n",
    "    print(f\"\\nHorizon h={h}:\")\n",
    "    print(f\"  Model MAE:     {result['model_mae']:.4f}\")\n",
    "    print(f\"  Persist. MAE:  {result['persist_mae']:.4f}\")\n",
    "    print(f\"  DM statistic:  {result['dm_stat']:.3f}\")\n",
    "    print(f\"  p-value:       {result['pvalue']:.4f}\")\n",
    "    print(f\"  Significant:   {'Yes' if result['significant'] else 'No'} (\u03b1=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pitfall Section\n",
    "\n",
    "### Pitfall 1: Using Naive t-test for Model Comparison\n",
    "\n",
    "```python\n",
    "# WRONG: Naive t-test ignores autocorrelation\n",
    "from scipy.stats import ttest_rel\n",
    "stat, pvalue = ttest_rel(errors_model, errors_persist)\n",
    "# p-value is too small \u2192 false significance!\n",
    "\n",
    "# RIGHT: DM test with HAC variance\n",
    "result = dm_test(errors_model, errors_persist, h=horizon)\n",
    "# p-value is correct\n",
    "```\n",
    "\n",
    "### Pitfall 2: Ignoring Horizon in HAC Bandwidth\n",
    "\n",
    "```python\n",
    "# WRONG: Default bandwidth for h=12 forecast\n",
    "result = dm_test(e1, e2, h=1)  # Pretending it's 1-step\n",
    "\n",
    "# RIGHT: Use correct horizon\n",
    "result = dm_test(e1, e2, h=12)  # Bandwidth will be 11\n",
    "```\n",
    "\n",
    "### Pitfall 3: Claiming Significance Without Adjustment\n",
    "\n",
    "```python\n",
    "# WRONG: Report raw improvement percentage\n",
    "improvement = (persist_mae - model_mae) / persist_mae * 100\n",
    "print(f\"Model is {improvement:.1f}% better!\")  # No significance test!\n",
    "\n",
    "# RIGHT: Report with statistical test\n",
    "result = dm_test(errors_model, errors_persist, h=horizon)\n",
    "if result.pvalue < 0.05:\n",
    "    print(f\"Model significantly better (DM p={result.pvalue:.3f})\")\n",
    "else:\n",
    "    print(f\"Improvement not significant (DM p={result.pvalue:.3f})\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the pitfalls\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "print(\"Pitfall Demonstrations\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Use the errors from earlier\n",
    "print(\"\\nPitfall 1: Naive t-test vs DM test\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Naive t-test (WRONG)\n",
    "loss_model = np.abs(errors_model)\n",
    "loss_persist = np.abs(errors_persist)\n",
    "stat_naive, pvalue_naive = ttest_rel(loss_model, loss_persist)\n",
    "print(f\"Naive t-test: stat={stat_naive:.3f}, p={pvalue_naive:.4f}\")\n",
    "\n",
    "# DM test (RIGHT)\n",
    "result_dm = dm_test(errors_model, errors_persist, h=4, loss='absolute')\n",
    "print(f\"DM test (h=4): stat={result_dm.statistic:.3f}, p={result_dm.pvalue:.4f}\")\n",
    "\n",
    "print(f\"\\n\u2192 Naive p-value: {pvalue_naive:.4f}\")\n",
    "print(f\"\u2192 DM p-value:    {result_dm.pvalue:.4f}\")\n",
    "print(f\"\\nThe naive test is anti-conservative (p-value too small).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### 1. h-Step Forecast Errors are MA(h-1) [T1]\n",
    "Multi-step forecast errors are autocorrelated up to lag h-1 by construction.\n",
    "\n",
    "### 2. Naive Variance Underestimates [T1]\n",
    "Assuming i.i.d. errors produces standard errors that are too small \u2192 false significance.\n",
    "\n",
    "### 3. HAC Variance Corrects the Problem [T1]\n",
    "The Newey-West estimator accounts for autocorrelation using the Bartlett kernel.\n",
    "\n",
    "### 4. DM Test for Model Comparison [T1]\n",
    "Use dm_test() with the correct horizon to properly compare forecast accuracy.\n",
    "\n",
    "### 5. Bandwidth = h - 1 [T1]\n",
    "For h-step forecasts, use bandwidth = h - 1 to capture the MA(h-1) error structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **05_shuffled_target_gate.ipynb**: Definitive leakage detection with permutation tests\n",
    "- **09_statistical_tests_dm_pt.ipynb**: Deep dive into DM and Pesaran-Timmermann tests\n",
    "- **08_validation_workflow.ipynb**: Complete HALT/WARN/PASS pipeline\n",
    "\n",
    "---\n",
    "\n",
    "*\"Never use naive standard errors for time series. Always use HAC variance.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
