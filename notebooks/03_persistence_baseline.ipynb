{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# The Persistence Baseline Problem\n\n## Why \"Low MAE\" Doesn't Mean Your Model is Good\n\n---\n\n## ðŸš¨ If You Know sklearn But Not Time Series Baselines, Read This First\n\n**What you already know (from standard ML)**:\n- Classification baseline: random chance (50% for binary)\n- Regression baseline: predict the mean\n- Lower error = better model\n- If you beat the baseline, you've learned something\n\n**What's different with time series**:\n\nIn time series, your baseline is **persistence**: \"tomorrow = today\"\n\n```python\n# Classification baseline: random guess\nbaseline = np.random.choice([0, 1], size=n)  # ~50% accuracy\n\n# Regression baseline: predict mean\nbaseline = np.full(n, y.mean())  # High MAE\n\n# TIME SERIES baseline: predict previous value\nbaseline = y[:-1]  # Can have VERY low MAE!\n```\n\n| Data Type | Baseline | Easy to Beat? |\n|-----------|----------|---------------|\n| Classification | Random (50%) | Yes |\n| Regression | Mean | Usually |\n| Low-persistence TS (Ï†=0.3) | Persistence | Yes |\n| High-persistence TS (Ï†=0.98) | Persistence | **Nearly impossible** |\n\n**The trap**: Your model has MAE = 0.05. Impressive! But persistence has MAE = 0.048.\nYou've learned to predict \"tomorrow = today\" â€” which is trivial and useless.\n\n---\n\n**What you'll learn:**\n1. Why persistence (naive forecast) is the natural baseline for time series\n2. Why persistence is nearly impossible to beat on high-autocorrelation data\n3. How to use MASE for scale-invariant evaluation\n4. How to detect \"too good to be true\" results with `gate_suspicious_improvement`\n\n**Prerequisites:** Notebooks 01-02\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from temporalcv.cv import WalkForwardCV\n",
    "from temporalcv.gates import gate_suspicious_improvement, GateStatus\n",
    "from temporalcv.metrics import compute_mase, compute_naive_error\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o2t7dif1r3",
   "source": "---\n\n## Section 0: Should You Even Try to Beat Persistence?\n\nBefore building a model, ask: **Is prediction even the right task?**\n\n### Decision Tree: When to Attempt Beating Persistence\n\n```\n1. Check ACF(1) of your series\n   â”‚\n   â”œâ”€ ACF(1) > 0.99 â”€â”€â”€> VERY HARD\n   â”‚   â”‚\n   â”‚   â””â”€ Theoretical max improvement: <0.02%\n   â”‚       Consider: Is prediction the right task?\n   â”‚       Alternative: Focus on direction or regime detection\n   â”‚\n   â”œâ”€ 0.95 < ACF(1) < 0.99 â”€â”€â”€> HARD\n   â”‚   â”‚\n   â”‚   â””â”€ Theoretical max improvement: <1%\n   â”‚       Use: Move-conditional metrics (MC-SS)\n   â”‚       Expect: MASE â‰ˆ 1.0 is normal\n   â”‚\n   â”œâ”€ 0.90 < ACF(1) < 0.95 â”€â”€â”€> DIFFICULT\n   â”‚   â”‚\n   â”‚   â””â”€ Theoretical max improvement: <2.5%\n   â”‚       Use: MASE as primary metric\n   â”‚       Expect: Small but meaningful gains possible\n   â”‚\n   â”œâ”€ 0.70 < ACF(1) < 0.90 â”€â”€â”€> MODERATE\n   â”‚   â”‚\n   â”‚   â””â”€ Theoretical max improvement: <10%\n   â”‚       Standard metrics (MAE, RMSE) are meaningful\n   â”‚       Models can add real value\n   â”‚\n   â””â”€ ACF(1) < 0.70 â”€â”€â”€> ACHIEVABLE\n       â”‚\n       â””â”€ Theoretical max improvement: >10%\n           Standard ML approaches work well\n           Persistence is a weak baseline\n```\n\n### Sample Size Guidance [T3]\n\n| Data Frequency | Minimum Observations | Minimum per CV Fold |\n|----------------|---------------------|---------------------|\n| Daily | 500+ | 100 |\n| Weekly | 104+ (2 years) | 52 |\n| Monthly | 60+ (5 years) | 24 |\n| Quarterly | 40+ (10 years) | 16 |\n\n**Rule of thumb**: `n_train >= 10 Ã— n_features` for stable estimates.\n\n**High persistence penalty**: When ACF(1) > 0.9, effective sample size is reduced. Consider 2Ã— the minimum observations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate AR(1) processes with different persistence levels\n",
    "def generate_ar1(n=500, phi=0.9, sigma=1.0, seed=42):\n",
    "    \"\"\"Generate AR(1) process with specified autocorrelation.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.zeros(n)\n",
    "    y[0] = rng.normal(0, sigma / np.sqrt(1 - phi**2))\n",
    "    for t in range(1, n):\n",
    "        y[t] = phi * y[t-1] + sigma * rng.normal()\n",
    "    return y\n",
    "\n",
    "def create_lag_features(series, n_lags=5):\n",
    "    \"\"\"Create lag features for prediction.\"\"\"\n",
    "    n = len(series)\n",
    "    X = np.column_stack([\n",
    "        np.concatenate([[np.nan]*lag, series[:-lag]]) \n",
    "        for lag in range(1, n_lags + 1)\n",
    "    ])\n",
    "    valid = ~np.isnan(X).any(axis=1)\n",
    "    return X[valid], series[valid]\n",
    "\n",
    "# Generate three series with different persistence\n",
    "series_low = generate_ar1(n=500, phi=0.3, seed=42)   # Low persistence\n",
    "series_mid = generate_ar1(n=500, phi=0.7, seed=42)   # Medium persistence\n",
    "series_high = generate_ar1(n=500, phi=0.98, seed=42) # High persistence (Treasury-like)\n",
    "\n",
    "print(f\"Low persistence (phi=0.3):  ACF(1) = {np.corrcoef(series_low[1:], series_low[:-1])[0,1]:.3f}\")\n",
    "print(f\"Medium persistence (phi=0.7): ACF(1) = {np.corrcoef(series_mid[1:], series_mid[:-1])[0,1]:.3f}\")\n",
    "print(f\"High persistence (phi=0.98): ACF(1) = {np.corrcoef(series_high[1:], series_high[:-1])[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: What is Persistence?\n",
    "\n",
    "**Persistence** (aka naive forecast) predicts:\n",
    "\n",
    "$$\\hat{y}_{t+1} = y_t$$\n",
    "\n",
    "In words: \"Tomorrow will be the same as today.\"\n",
    "\n",
    "For changes: persistence predicts zero change:\n",
    "\n",
    "$$\\hat{\\Delta y}_{t+1} = 0$$\n",
    "\n",
    "**This is your baseline.** Any model must beat this to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate what persistence means\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Left: Level prediction\n",
    "ax = axes[0]\n",
    "t = np.arange(50)\n",
    "sample = series_high[:50]\n",
    "persistence_pred = np.roll(sample, 1)[1:]  # y[t+1] = y[t]\n",
    "\n",
    "ax.plot(t, sample, 'b-', linewidth=2, label='Actual', alpha=0.8)\n",
    "ax.plot(t[1:], persistence_pred, 'r--', linewidth=2, label='Persistence (y[t-1])', alpha=0.8)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Persistence on Levels: y_hat[t] = y[t-1]', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Right: Change prediction\n",
    "ax = axes[1]\n",
    "changes = np.diff(sample)\n",
    "persistence_change = np.zeros(len(changes))  # Persistence predicts 0 change\n",
    "\n",
    "ax.plot(changes, 'b-', linewidth=2, label='Actual change', alpha=0.8)\n",
    "ax.axhline(y=0, color='r', linestyle='--', linewidth=2, label='Persistence (0)')\n",
    "ax.fill_between(range(len(changes)), changes, 0, alpha=0.3)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Change (Î”y)')\n",
    "ax.set_title('Persistence on Changes: Î”y_hat = 0', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPersistence MAE on changes: {np.mean(np.abs(changes)):.4f}\")\n",
    "print(f\"(This IS the baseline to beat)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Why Persistence is Hard to Beat [T1]\n",
    "\n",
    "### The Mathematics of Persistence\n",
    "\n",
    "For an AR(1) process with autocorrelation $\\phi$:\n",
    "\n",
    "$$y_{t+1} = \\phi \\cdot y_t + \\epsilon_{t+1}$$\n",
    "\n",
    "The **optimal 1-step forecast** is:\n",
    "\n",
    "$$\\hat{y}_{t+1}^* = \\phi \\cdot y_t$$\n",
    "\n",
    "The **persistence forecast** is:\n",
    "\n",
    "$$\\hat{y}_{t+1}^{\\text{persist}} = y_t$$\n",
    "\n",
    "The **difference in expected squared error**:\n",
    "\n",
    "$$\\text{MSE}_{\\text{persist}} - \\text{MSE}_{\\text{optimal}} = (1-\\phi)^2 \\cdot \\text{Var}(y)$$\n",
    "\n",
    "When $\\phi \\to 1$, this difference approaches **zero**.\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "- **Low phi (0.3)**: Tomorrow is mostly noise â†’ persistence is bad â†’ easy to beat\n",
    "- **High phi (0.98)**: Tomorrow is almost the same as today â†’ persistence is great â†’ nearly impossible to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the theoretical bound: as phi -> 1, persistence becomes optimal\n",
    "phi_values = np.linspace(0.1, 0.99, 50)\n",
    "\n",
    "# Theoretical improvement possible over persistence (as fraction of variance)\n",
    "improvement_possible = (1 - phi_values)**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(phi_values, improvement_possible * 100, 'b-', linewidth=2)\n",
    "ax.fill_between(phi_values, 0, improvement_possible * 100, alpha=0.3)\n",
    "\n",
    "# Mark common phi values\n",
    "for phi, label in [(0.3, 'Low'), (0.7, 'Medium'), (0.98, 'High\\n(Treasury)')]:\n",
    "    improvement = (1 - phi)**2 * 100\n",
    "    ax.axvline(x=phi, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.scatter([phi], [improvement], color='red', s=100, zorder=5)\n",
    "    ax.annotate(f'{label}\\n{improvement:.1f}%', xy=(phi, improvement),\n",
    "                xytext=(phi + 0.03, improvement + 5), fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Autocorrelation (Ï†)', fontsize=12)\n",
    "ax.set_ylabel('Maximum Possible Improvement\\nover Persistence (%)', fontsize=12)\n",
    "ax.set_title('[T1] Theoretical Bound: High Ï† = Hard to Beat Persistence', \n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(f\"  At Ï†=0.3: Up to {(1-0.3)**2*100:.1f}% improvement is theoretically possible\")\n",
    "print(f\"  At Ï†=0.98: Only {(1-0.98)**2*100:.2f}% improvement is theoretically possible\")\n",
    "print(f\"\\n  Claims of >20% improvement on high-Ï† data are SUSPICIOUS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical demonstration: train models on different phi series\n",
    "def evaluate_vs_persistence(series, phi_label):\n",
    "    \"\"\"Train a model and compare to persistence.\"\"\"\n",
    "    X, y = create_lag_features(series, n_lags=5)\n",
    "    \n",
    "    # Train-test split (80-20)\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Model predictions\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Persistence predictions (last known value)\n",
    "    persistence_preds = X_test[:, 0]  # First lag = y[t-1]\n",
    "    \n",
    "    # Calculate MAEs\n",
    "    model_mae = mean_absolute_error(y_test, preds)\n",
    "    persistence_mae = mean_absolute_error(y_test, persistence_preds)\n",
    "    \n",
    "    # Improvement\n",
    "    improvement = (persistence_mae - model_mae) / persistence_mae * 100\n",
    "    \n",
    "    return {\n",
    "        'phi': phi_label,\n",
    "        'model_mae': model_mae,\n",
    "        'persistence_mae': persistence_mae,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "# Test all three series\n",
    "results = [\n",
    "    evaluate_vs_persistence(series_low, 0.3),\n",
    "    evaluate_vs_persistence(series_mid, 0.7),\n",
    "    evaluate_vs_persistence(series_high, 0.98),\n",
    "]\n",
    "\n",
    "print(\"MODEL vs PERSISTENCE BASELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Ï†':<10} {'Model MAE':<15} {'Persist. MAE':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    print(f\"{r['phi']:<10} {r['model_mae']:<15.4f} {r['persistence_mae']:<15.4f} {r['improvement']:>+.1f}%\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nAs Ï† increases, beating persistence becomes nearly impossible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: The MAE Mirage\n",
    "\n",
    "**The Problem:** Your model might have impressively low MAE, but so does persistence!\n",
    "\n",
    "Low absolute error doesn't mean the model has learned anything. What matters is whether you beat the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the MAE mirage\n",
    "X_high, y_high = create_lag_features(series_high, n_lags=5)\n",
    "\n",
    "# Train-test split\n",
    "split_idx = int(len(X_high) * 0.8)\n",
    "X_train, X_test = X_high[:split_idx], X_high[split_idx:]\n",
    "y_train, y_test = y_high[:split_idx], y_high[split_idx:]\n",
    "\n",
    "# Train model\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "model_preds = model.predict(X_test)\n",
    "\n",
    "# Persistence predictions\n",
    "persistence_preds = X_test[:, 0]\n",
    "\n",
    "# Compare\n",
    "model_mae = mean_absolute_error(y_test, model_preds)\n",
    "persistence_mae = mean_absolute_error(y_test, persistence_preds)\n",
    "\n",
    "print(\"THE MAE MIRAGE\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nHigh-persistence series (Ï†=0.98, like Treasury rates)\")\n",
    "print(f\"\\n  Model MAE:       {model_mae:.4f}  <-- Looks great!\")\n",
    "print(f\"  Persistence MAE: {persistence_mae:.4f}  <-- Also great...\")\n",
    "print(f\"\\n  Improvement: {(persistence_mae - model_mae) / persistence_mae * 100:.1f}%\")\n",
    "print(f\"\\n  The model's 'impressive' MAE is just matching persistence.\")\n",
    "print(f\"  It learned to predict 'tomorrow = today', which is trivial.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: model predictions vs persistence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Predictions over time\n",
    "ax = axes[0]\n",
    "t = np.arange(len(y_test))\n",
    "ax.plot(t, y_test, 'b-', linewidth=2, label='Actual', alpha=0.8)\n",
    "ax.plot(t, model_preds, 'g--', linewidth=2, label='Model', alpha=0.8)\n",
    "ax.plot(t, persistence_preds, 'r:', linewidth=2, label='Persistence', alpha=0.8)\n",
    "ax.set_xlabel('Test Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Model vs Persistence: Nearly Identical!', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Right: Model predictions vs persistence predictions\n",
    "ax = axes[1]\n",
    "ax.scatter(persistence_preds, model_preds, alpha=0.5, s=30)\n",
    "min_val = min(persistence_preds.min(), model_preds.min())\n",
    "max_val = max(persistence_preds.max(), model_preds.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='y=x')\n",
    "ax.set_xlabel('Persistence Prediction', fontsize=11)\n",
    "ax.set_ylabel('Model Prediction', fontsize=11)\n",
    "ax.set_title('Model Learned to Mimic Persistence', fontsize=12, fontweight='bold', color='red')\n",
    "ax.legend()\n",
    "\n",
    "# Calculate correlation\n",
    "corr = np.corrcoef(persistence_preds, model_preds)[0, 1]\n",
    "ax.annotate(f'Correlation: {corr:.3f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "            fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrelation between model and persistence: {corr:.3f}\")\n",
    "print(f\"The model is essentially just predicting y[t+1] = y[t].\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: MASE â€” The Scale-Invariant Answer [T1]\n",
    "\n",
    "**MASE (Mean Absolute Scaled Error)** normalizes MAE by the persistence baseline:\n",
    "\n",
    "$$\\text{MASE} = \\frac{\\text{MAE}}{\\text{MAE}_{\\text{naive}}}$$\n",
    "\n",
    "Where $\\text{MAE}_{\\text{naive}}$ is the in-sample MAE of the persistence forecast.\n",
    "\n",
    "**Interpretation:**\n",
    "- MASE < 1: Model beats persistence â†’ **Good!**\n",
    "- MASE = 1: Model equals persistence â†’ Learned nothing\n",
    "- MASE > 1: Model is worse than persistence â†’ **Bad!**\n",
    "\n",
    "### Reference [T1]\n",
    "Hyndman & Koehler (2006): \"Another look at measures of forecast accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MASE for all three series\n",
    "def evaluate_with_mase(series, phi_label):\n",
    "    \"\"\"Evaluate using MASE.\"\"\"\n",
    "    X, y = create_lag_features(series, n_lags=5)\n",
    "    \n",
    "    # Train-test split\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Naive MAE from training data\n",
    "    naive_mae = compute_naive_error(y_train, method='persistence')\n",
    "    \n",
    "    # Model predictions\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # MASE\n",
    "    mase = compute_mase(preds, y_test, naive_mae)\n",
    "    \n",
    "    return {\n",
    "        'phi': phi_label,\n",
    "        'mae': mean_absolute_error(y_test, preds),\n",
    "        'naive_mae': naive_mae,\n",
    "        'mase': mase\n",
    "    }\n",
    "\n",
    "mase_results = [\n",
    "    evaluate_with_mase(series_low, 0.3),\n",
    "    evaluate_with_mase(series_mid, 0.7),\n",
    "    evaluate_with_mase(series_high, 0.98),\n",
    "]\n",
    "\n",
    "print(\"MASE EVALUATION\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Ï†':<10} {'Model MAE':<15} {'Naive MAE':<15} {'MASE':<15} {'Verdict'}\")\n",
    "print(\"-\" * 65)\n",
    "for r in mase_results:\n",
    "    verdict = 'âœ“ Beats baseline' if r['mase'] < 1 else 'âœ— No skill'\n",
    "    print(f\"{r['phi']:<10} {r['mae']:<15.4f} {r['naive_mae']:<15.4f} {r['mase']:<15.3f} {verdict}\")\n",
    "print(\"-\" * 65)\n",
    "print(\"\\nMASE reveals the truth: high-Ï† series don't improve over persistence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MASE across different phi levels\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "phis = [r['phi'] for r in mase_results]\n",
    "mases = [r['mase'] for r in mase_results]\n",
    "\n",
    "colors = ['green' if m < 1 else 'red' for m in mases]\n",
    "bars = ax.bar(range(len(phis)), mases, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Add baseline line at MASE=1\n",
    "ax.axhline(y=1, color='black', linestyle='--', linewidth=2, label='Persistence baseline')\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks(range(len(phis)))\n",
    "ax.set_xticklabels([f'Ï†={p}' for p in phis])\n",
    "ax.set_ylabel('MASE', fontsize=12)\n",
    "ax.set_title('MASE by Persistence Level\\n(< 1 = Better than persistence)', \n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, mase) in enumerate(zip(bars, mases)):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{mase:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_ylim(0, max(mases) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: `gate_suspicious_improvement`\n",
    "\n",
    "temporalcv provides `gate_suspicious_improvement` to automatically flag results that are \"too good to be true.\"\n",
    "\n",
    "**Thresholds [T2/T3]:**\n",
    "- >20% improvement over baseline: **HALT** â€” likely leakage\n",
    "- 10-20% improvement: **WARN** â€” investigate\n",
    "- <10% improvement: **PASS** â€” plausible\n",
    "\n",
    "**Why these thresholds?**\n",
    "- Per theoretical bounds, >20% improvement is nearly impossible for high-Ï† data\n",
    "- In practice, >20% improvements usually indicate leakage or bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gate_suspicious_improvement\n",
    "print(\"gate_suspicious_improvement Demo\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Scenario 1: Realistic improvement (5%)\n",
    "model_mae_realistic = 0.95  # 5% better\n",
    "baseline_mae = 1.0\n",
    "\n",
    "result1 = gate_suspicious_improvement(\n",
    "    model_metric=model_mae_realistic,\n",
    "    baseline_metric=baseline_mae\n",
    ")\n",
    "print(f\"\\nScenario 1: 5% improvement\")\n",
    "print(f\"  Model MAE: {model_mae_realistic}, Baseline: {baseline_mae}\")\n",
    "print(f\"  Status: {result1.status.value}\")\n",
    "print(f\"  Message: {result1.message}\")\n",
    "\n",
    "# Scenario 2: Suspicious improvement (25%)\n",
    "model_mae_suspicious = 0.75  # 25% better\n",
    "\n",
    "result2 = gate_suspicious_improvement(\n",
    "    model_metric=model_mae_suspicious,\n",
    "    baseline_metric=baseline_mae\n",
    ")\n",
    "print(f\"\\nScenario 2: 25% improvement\")\n",
    "print(f\"  Model MAE: {model_mae_suspicious}, Baseline: {baseline_mae}\")\n",
    "print(f\"  Status: {result2.status.value}\")\n",
    "print(f\"  Message: {result2.message}\")\n",
    "if result2.recommendation:\n",
    "    print(f\"  Recommendation: {result2.recommendation}\")\n",
    "\n",
    "# Scenario 3: Edge case (15%)\n",
    "model_mae_warn = 0.85  # 15% better\n",
    "\n",
    "result3 = gate_suspicious_improvement(\n",
    "    model_metric=model_mae_warn,\n",
    "    baseline_metric=baseline_mae\n",
    ")\n",
    "print(f\"\\nScenario 3: 15% improvement\")\n",
    "print(f\"  Model MAE: {model_mae_warn}, Baseline: {baseline_mae}\")\n",
    "print(f\"  Status: {result3.status.value}\")\n",
    "print(f\"  Message: {result3.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gate on our actual results\n",
    "print(\"Applying gate_suspicious_improvement to our models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for r in results:  # From Section 2\n",
    "    gate_result = gate_suspicious_improvement(\n",
    "        model_metric=r['model_mae'],\n",
    "        baseline_metric=r['persistence_mae']\n",
    "    )\n",
    "    \n",
    "    status_color = {\n",
    "        'PASS': 'âœ“',\n",
    "        'WARN': 'âš ',\n",
    "        'HALT': 'âœ—'\n",
    "    }.get(gate_result.status.value, '?')\n",
    "    \n",
    "    print(f\"\\nÏ†={r['phi']}:\")\n",
    "    print(f\"  Improvement: {r['improvement']:.1f}%\")\n",
    "    print(f\"  Gate status: {status_color} {gate_result.status.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Complete Evaluation Workflow\n",
    "\n",
    "Always evaluate with both:\n",
    "1. **MASE** â€” to see if you beat persistence\n",
    "2. **gate_suspicious_improvement** â€” to catch \"too good\" results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_forecast_properly(series, horizon=1, n_lags=5, random_state=42):\n    \"\"\"\n    Complete forecast evaluation with MASE and gate checks.\n    \n    Parameters\n    ----------\n    series : array-like\n        Time series to forecast\n    horizon : int\n        Forecast horizon\n    n_lags : int\n        Number of lag features\n    random_state : int\n        Random seed\n        \n    Returns\n    -------\n    dict\n        Comprehensive evaluation results\n    \"\"\"\n    np.random.seed(random_state)\n    \n    # Prepare data\n    X, y = create_lag_features(series, n_lags=n_lags)\n    \n    # Walk-forward CV with proper gap\n    cv = WalkForwardCV(\n        n_splits=5,\n        extra_gap=horizon,\n        window_type='expanding',\n        test_size=50\n    )\n    \n    model_maes = []\n    persistence_maes = []\n    all_preds = []\n    all_actuals = []\n    all_train_data = []  # Collect training data for MASE denominator\n    \n    for train_idx, test_idx in cv.split(X):\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_test, y_test = X[test_idx], y[test_idx]\n        \n        # Train model\n        model = Ridge(alpha=1.0)\n        model.fit(X_train, y_train)\n        preds = model.predict(X_test)\n        \n        # Persistence predictions\n        persist_preds = X_test[:, 0]  # y[t-1]\n        \n        # Collect errors\n        model_maes.append(mean_absolute_error(y_test, preds))\n        persistence_maes.append(mean_absolute_error(y_test, persist_preds))\n        all_preds.extend(preds)\n        all_actuals.extend(y_test)\n        all_train_data.extend(y_train)  # Collect all training targets\n    \n    # Aggregate\n    model_mae = np.mean(model_maes)\n    persistence_mae = np.mean(persistence_maes)\n    \n    # MASE: naive error from TRAINING data per Hyndman & Koehler 2006\n    # Using collected training data instead of full series to avoid leakage\n    naive_mae = compute_naive_error(np.array(all_train_data), method='persistence')\n    mase = compute_mase(np.array(all_preds), np.array(all_actuals), naive_mae)\n    \n    # Gate check\n    gate_result = gate_suspicious_improvement(\n        model_metric=model_mae,\n        baseline_metric=persistence_mae\n    )\n    \n    return {\n        'model_mae': model_mae,\n        'persistence_mae': persistence_mae,\n        'improvement_pct': (persistence_mae - model_mae) / persistence_mae * 100,\n        'mase': mase,\n        'beats_persistence': mase < 1,\n        'gate_status': gate_result.status.value,\n        'gate_message': gate_result.message\n    }\n\n# Run complete evaluation\nprint(\"COMPLETE FORECAST EVALUATION\")\nprint(\"=\" * 70)\n\nfor series, phi in [(series_low, 0.3), (series_mid, 0.7), (series_high, 0.98)]:\n    result = evaluate_forecast_properly(series)\n    \n    print(f\"\\nÏ†={phi}:\")\n    print(f\"  Model MAE:      {result['model_mae']:.4f}\")\n    print(f\"  Persist. MAE:   {result['persistence_mae']:.4f}\")\n    print(f\"  Improvement:    {result['improvement_pct']:+.1f}%\")\n    print(f\"  MASE:           {result['mase']:.3f} ({'âœ“ beats baseline' if result['beats_persistence'] else 'âœ— no skill'})\")\n    print(f\"  Gate Status:    {result['gate_status']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pitfall Section\n",
    "\n",
    "### Pitfall 1: Reporting MAE Without Context\n",
    "\n",
    "```python\n",
    "# WRONG: MAE alone is meaningless for time series\n",
    "print(f\"Model MAE: {mae:.4f}\")  # Looks impressive!\n",
    "\n",
    "# RIGHT: Always compare to persistence\n",
    "print(f\"Model MAE: {model_mae:.4f}\")\n",
    "print(f\"Persistence MAE: {persistence_mae:.4f}\")\n",
    "print(f\"MASE: {mase:.3f}\")\n",
    "```\n",
    "\n",
    "### Pitfall 2: Claiming \"X% Improvement\" Without Verification\n",
    "\n",
    "```python\n",
    "# WRONG: Trust the improvement\n",
    "improvement = 25%  # Great!\n",
    "\n",
    "# RIGHT: Verify with gate\n",
    "result = gate_suspicious_improvement(model_mae, persistence_mae)\n",
    "if result.status == GateStatus.HALT:\n",
    "    raise ValueError(f\"Suspicious improvement: {result.message}\")\n",
    "```\n",
    "\n",
    "### Pitfall 3: Using RMSE Instead of MAE for MASE\n",
    "\n",
    "```python\n",
    "# WRONG: MASE uses MAE, not RMSE\n",
    "mase = rmse / naive_rmse  # Incorrect!\n",
    "\n",
    "# RIGHT: MASE definition\n",
    "mase = mae / naive_mae  # Correct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the pitfalls\n",
    "print(\"Pitfall Demonstrations\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Pitfall 1: MAE without context\n",
    "model_mae = 0.05\n",
    "print(\"\\nPitfall 1: MAE without context\")\n",
    "print(f\"  'Model MAE: {model_mae}'\")\n",
    "print(f\"  â†‘ Is this good? You have no idea without persistence baseline!\")\n",
    "print(f\"  â†“ With context:\")\n",
    "print(f\"     Model MAE: {model_mae}\")\n",
    "print(f\"     Persistence MAE: 0.048\")\n",
    "print(f\"     MASE: 1.04 (worse than persistence!)\")\n",
    "\n",
    "# Pitfall 2: Trusting big improvements\n",
    "print(\"\\nPitfall 2: Trusting big improvements\")\n",
    "print(f\"  '25% improvement!' â†’ Run gate_suspicious_improvement first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "### 1. Persistence is the Natural Baseline [T1]\n",
    "For time series, \"no change\" is the simplest forecast. All models must beat this.\n",
    "\n",
    "### 2. High-Ï† Data is Hard to Beat [T1]\n",
    "When autocorrelation is high (Ï† > 0.9), the theoretical improvement over persistence approaches zero.\n",
    "\n",
    "### 3. Low MAE â‰  Good Model\n",
    "A model can have low MAE but no skill â€” it just learned to predict \"tomorrow = today.\"\n",
    "\n",
    "### 4. MASE Reveals the Truth [T1]\n",
    "MASE < 1 means better than persistence. MASE â‰¥ 1 means no skill.\n",
    "\n",
    "### 5. >20% Improvement is Suspicious [T2]\n",
    "For high-persistence data, large improvements usually indicate leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **04_autocorrelation_matters.ipynb**: HAC variance for correlated forecast errors\n",
    "- **05_shuffled_target_gate.ipynb**: Definitive leakage detection\n",
    "- **10_high_persistence_metrics.ipynb**: MC-SS and move-conditional evaluation\n",
    "\n",
    "---\n",
    "\n",
    "*\"A low MAE is meaningless without a baseline. Always compute MASE.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}