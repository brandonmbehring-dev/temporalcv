# Comprehensive Audit: temporalcv

**Date**: 2025-12-24
**Reviewer**: Claude (Opus 4.5)
**Version Audited**: 1.0.0-rc1
**Status**: Pre-Publication Review

---

## Finalized Implementation Decisions (via /iterate)

### Round 1: Critical & High Priority

| # | Issue | Decision | Impact |
|---|-------|----------|--------|
| 1 | n_shuffles=5 power problem | Add `strict=True` parameter (n=99 when strict, n=5 default) | Backward compatible, prominent docs |
| 2 | Missing .tracking/ | Full hub compliance: decisions.md, technical_debt.yaml, phase_transitions.log | 15 min setup |
| 3 | Conformal spec/code drift | Tiered: n>=10 allowed, warn at <30, n>=50 for reliable | Update SPECIFICATION.md + code warning |
| 4 | Benchmark comparison | Full suite with published results vs sklearn/sktime/Darts | 4-6 hours, table in README |
| 5 | Property tests | Full Hypothesis suite: gates, CV, metrics invariants | 2-3 hours, 3 new test files |
| 6 | DM test limitations | Docstring + docs page + runtime warnings | Comprehensive protection |

### Round 2: Medium & Low Priority

| # | Issue | Decision | Impact |
|---|-------|----------|--------|
| 7 | Self-normalized DM test | v1.1 roadmap (skip for v1.0) | Document as future work |
| 8 | requirements.lock | pip-compile without hashes | Simple reproducibility |
| 9 | Quickstart example | All three: .py + README + .ipynb | Comprehensive onboarding (~1 hour) |
| 10 | Citation format | CITATION.cff + README BibTeX | GitHub native + visible in README |

---

## Executive Summary

**Overall Assessment**: temporalcv is **publication-ready** with high code quality, exceptional test coverage, and rigorous statistical foundations. However, several methodological gaps and unstated assumptions warrant attention before v1.0 release.

| Dimension | Score | Status |
|-----------|-------|--------|
| Code Quality | 9.2/10 | Excellent |
| Test Coverage | 9.6/10 | Exceptional |
| Documentation | 8.5/10 | Strong, minor gaps |
| ML Methodology | 7.5/10 | **Needs attention** |
| Benchmarking | 6.5/10 | **Significant gaps** |
| Statistical Rigor | 8.8/10 | Strong, some concerns |

---

## 1. Code Quality Assessment

### 1.1 Strengths

1. **Exceptional type coverage**: 94.6% return type hints, 93.7% docstring coverage
2. **No silent failures**: 242 explicit error checks across codebase (CLAUDE.md principle enforced)
3. **Consistent API**: sklearn-compatible interfaces (`BaseCrossValidator`, `Protocol` patterns)
4. **Clean architecture**: 44 modules with clear separation of concerns
5. **Knowledge tier system**: [T1/T2/T3] tags provide transparency on claim confidence

### 1.2 Issues Identified

| Issue | Location | Severity | Recommendation |
|-------|----------|----------|----------------|
| Silent fallback in `get_n_splits()` | `cv.py:695-704` | Medium | Replace `except ValueError: return 0` with explicit error |
| Long method (167 lines) | `cv_financial.py:_apply_purge_and_embargo()` | Low | Refactor into 2-3 helper methods |
| Inconsistent input validation | Across metric submodules | Low | Export shared `_validate_inputs()` from `metrics/__init__.py` |
| 6 functions lack "optional" keyword | `gates.py`, `conformal.py` | Low | Add explicit optional markers to parameter docs |

### 1.3 Missing Type Hints (35% of parameters)

While return types are well-covered (94.6%), parameter type hints cover only 65%. For a publishable package, recommend raising to 85%+.

**Files needing attention**:
- `regimes.py` - 3 functions missing parameter hints
- `conformal.py` - `random_state` parameter undocumented as Optional
- `gates.py:run_gates_stratified` - `regimes` parameter type unclear

---

## 2. Test Suite Analysis

### 2.1 Strengths (Exceptional Quality)

| Category | Count | Assessment |
|----------|-------|------------|
| Test functions | 1,338+ | Comprehensive |
| Test files | 51 | Well-organized |
| Property-based tests | 5 files (Hypothesis) | Professional |
| Anti-pattern tests | 2 files | Critical for leakage detection |
| Monte Carlo tests | 3 files | Statistical calibration verified |
| Edge case tests | 477 lines | Thorough |
| Reproducibility tests | 318 lines (20 parametrized seeds) | Guarantees determinism |

**No skipped tests or TODOs** - exceptional discipline.

### 2.2 Gaps Identified

| Gap | Impact | Recommendation |
|-----|--------|----------------|
| No stress tests (n > 10,000) | Medium | Add `@pytest.mark.slow` tests with n=50,000+ |
| Missing extreme AR parameters | Low | Add tests for `phi=-0.99`, `sigma=0.001` |
| No negative variance test for DM | Medium | Test HAC with strong negative autocorrelation |
| Missing error message validation | Low | Test that error messages are actionable |

### 2.3 Critical Anti-Pattern Coverage

**Verified coverage of CLAUDE.md bug categories**:
- Bug #1 (Lag leakage): `test_lag_leakage.py` - **COVERED**
- Bug #2 (Boundary violations): `test_boundary_violations.py` - **COVERED**
- Bugs #3-10: Gate framework tests - **COVERED**

---

## 3. Documentation Review

### 3.1 Strengths

- 35 markdown files with consistent structure
- Mathematical foundations with full derivations and [T1] citations
- SPECIFICATION.md with amendment process (mature governance)
- 5 worked examples (~1,900 lines)
- Knowledge tier system consistently applied (~95%)

### 3.2 Critical Gaps

| Gap | Severity | Location | Recommendation |
|-----|----------|----------|----------------|
| MC-SS formula incomplete | High | `mathematical_foundations.md:217-264` | Add formal skill score definition |
| 3-class PT test decision rule | High | `SPECIFICATION.md:147-154` | When to use vs 2-class? Add guidance |
| n_shuffles=5 power analysis | Medium | `SPECIFICATION.md:33-34` | Document required shuffles for power |
| Missing BibTeX | Medium | README.md | Add citation format for academic users |
| No "Add Your Own Data" guide | Medium | docs/tutorials/ | Create onboarding tutorial |

### 3.3 Docstring Quality Issues

**Functions with incomplete docstrings** (6 identified):
1. `gates.py:run_gates_stratified` - `regimes` parameter
2. `conformal.py:AdaptiveConformalPredictor` - `random_state`
3. `regimes.py` - 3 functions with unclear defaults

---

## 4. Benchmarking Infrastructure

### 4.1 Current State

**Loaders implemented** (5):
- `fred.py` - FRED API (requires key)
- `m5.py` - Walmart M5 (requires manual download)
- `gluonts.py` - GluonTS (electricity, traffic)
- `monash.py` - Monash archive (M3, M4)
- `base.py` - Synthetic AR(1)

**Benchmark tests exist** (`tests/benchmarks/`):
- `test_cv_benchmarks.py` - 281 lines (performance only)
- `test_gate_benchmarks.py`
- `test_metric_benchmarks.py`

### 4.2 Critical Gaps

| Gap | Severity | Impact |
|-----|----------|--------|
| **No benchmark runner** | HIGH | Users cannot reproduce package claims |
| **No comparison table** | HIGH | Cannot validate vs sklearn/sktime/Darts |
| **No local-reproducible dataset** | HIGH | FRED requires API key, M5 requires download |
| Only synthetic data works offline | Medium | Limits CI/CD testing |
| No benchmark results published | Medium | README claims lack evidence |

### 4.3 Recommendations

1. **Create `temporalcv.benchmarks.run_benchmarks()`**:
   ```python
   def run_benchmarks(
       datasets: list[str] = ["synthetic_ar1", "electricity"],
       methods: list[str] = ["walk_forward", "sklearn_timeseries"],
       output: str = "results/benchmark_results.csv"
   ) -> pd.DataFrame:
       """Run comparative benchmarks and return results table."""
   ```

2. **Add comparison vs competitors**:
   - scikit-learn's `TimeSeriesSplit`
   - sktime's `ExpandingWindowSplitter`
   - Darts' `HistoricalForecasts`

3. **Include reproducible local dataset** (synthetic electricity-like)

---

## 5. ML Methodology Critique

### 5.1 Unstated Assumptions [T3]

| Assumption | Location | Risk | Recommendation |
|------------|----------|------|----------------|
| **20% improvement = "too good"** | `SPECIFICATION.md:16` | May be domain-specific | Add sensitivity analysis or domain customization |
| **n_shuffles=5 sufficient** | `SPECIFICATION.md:34` | Low power for marginal effects | Document required shuffles for 80% power |
| **70th percentile for moves** | `SPECIFICATION.md:81` | Arbitrary cutoff | Justify via move frequency analysis |
| **13-week volatility window** | `SPECIFICATION.md:246` | Financial-specific | Document domain dependency |
| **phi=0.95 "typical"** | `SPECIFICATION.md:48` | Varies by domain | Add calibration guidance |
| **tolerance=1.5 for AR(1)** | `SPECIFICATION.md:50` | Finite-sample magic number | Derive from theory or Monte Carlo |

### 5.2 Methodological Concerns

#### 5.2.1 Shuffled Target Test Power

**Issue**: `n_shuffles=5` provides limited statistical power.

**Analysis**: With 5 shuffles, the minimum achievable p-value is 1/6 ~ 0.167, **above the 0.05 threshold**. The test can only detect *very* strong leakage.

**Recommendation**:
- Increase default to `n_shuffles=99` (minimum p-value = 0.01)
- Or use permutation test with bootstrap for p-value estimation
- Document power analysis in `SPECIFICATION.md`

**Reference**: [Phipson & Smyth (2010)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2687965/) on permutation test power.

#### 5.2.2 DM Test Limitations

**Issue**: HAC variance estimation has well-documented finite-sample problems.

From [Diebold's 2015 retrospective](https://www.nber.org/system/files/working_papers/w18391/w18391.pdf):
> "The DM test was intended for comparing *forecasts* and remains useful in that regard. The DM test was *not* intended for comparing *models*."

**Concerns**:
1. **Negative variance estimates** possible with multi-step forecasts ([Coroneo & Iacone 2016](https://www.sciencedirect.com/science/article/abs/pii/S0165176517300575))
2. **Low power** when autocorrelation is strong
3. **Size distortions** in small samples even with Harvey adjustment

**Recommendations**:
- Add warning when bandwidth > n/4
- Consider implementing weighted periodogram estimator for h > 1
- Document these limitations prominently
- Add `dm_test(..., method="robust")` option using [self-normalized test](https://arxiv.org/abs/2510.04318)

#### 5.2.3 Conformal Prediction Exchangeability

**Issue**: Standard conformal prediction assumes exchangeability, which is **violated in time series**.

From [A Gentle Introduction to Conformal Time Series Forecasting](https://arxiv.org/html/2511.13608):
> "These guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data."

**Current implementation**: `AdaptiveConformalPredictor` addresses this, but:
- `gamma=0.1` default lacks theoretical justification
- No guidance on when split conformal is safe vs when adaptive is required
- Missing coverage diagnostics

**Recommendations**:
1. Add `check_exchangeability()` diagnostic
2. Document when to use split vs adaptive conformal
3. Consider implementing [Bellman Conformal Inference](https://arxiv.org/abs/2402.05203) for optimal adaptation
4. Add coverage monitoring to detect miscalibration

#### 5.2.4 3-Class PT Test

**Issue**: The 3-class Pesaran-Timmermann extension is marked [T3] but lacks decision criteria.

**Current documentation**: "Use 2-class mode for rigorous statistical testing."

**Problem**: No guidance on when 3-class IS appropriate.

**Recommendation**: Add decision rule:
```
Use 3-class mode ONLY when:
1. Directional prediction has 3 explicit categories (UP/DOWN/FLAT)
2. You care about distinguishing FLAT from incorrect direction
3. You accept higher Type I error rate (approximate variance)

Otherwise: Use 2-class mode (threshold predictions at 0).
```

### 5.3 Missing Methodological Features

| Feature | Rationale | Priority |
|---------|-----------|----------|
| **Nested CV** | Hyperparameter tuning without leakage | High |
| **Multi-step forecast CV** | h=1,2,3,4 comparison | Medium |
| **Cross-validation for model selection** | Beyond just evaluation | Medium |
| **Blocked bootstrap CI for gates** | Uncertainty quantification for gate decisions | Medium |

---

## 6. Statistical Methods Review

### 6.1 Verified Correct [T1]

| Method | Implementation | Reference |
|--------|---------------|-----------|
| DM test statistic | Correct | Diebold & Mariano (1995) |
| Harvey adjustment | Correct | Harvey et al. (1997) |
| HAC variance (Bartlett) | Correct | Newey & West (1987) |
| PT test (2-class) | Correct | Pesaran & Timmermann (1992) |
| Split conformal quantile | Correct | Romano et al. (2019) |
| Adaptive conformal update | Correct | Gibbs & Candes (2021) |
| AR(1) optimal MAE derivation | Correct | Standard statistics |

### 6.2 Concerns

| Method | Concern | Recommendation |
|--------|---------|----------------|
| HAC bandwidth selection | Andrews (1991) rule may be suboptimal for forecast errors | Consider h-dependent bandwidth: `bandwidth = h - 1` |
| PT test variance formula | Complex approximation - verify numerically | Add Monte Carlo validation test |
| Block bootstrap block size | n^(1/3) heuristic may be conservative | Add data-driven selection option |

### 6.3 Missing Statistical Features

| Feature | Importance | Reference |
|---------|------------|-----------|
| Giacomini-White test | Conditional predictive ability | Giacomini & White (2006) |
| Clark-West test | Nested model comparison | Clark & West (2007) |
| Reality check / SPA test | Multiple model comparison | Hansen (2005) |
| Forecast encompassing test | Model combination | Harvey et al. (1998) |

---

## 7. Examples and Notebooks

### 7.1 Current State

**Examples** (5 files, ~1,900 lines):
- `01_leakage_detection.py` - 387 lines, FRED fallback
- `02_walk_forward_cv.py` - 394 lines
- `03_statistical_tests.py` - 430 lines
- `04_high_persistence.py` - 285 lines
- `05_conformal_prediction.py` - 450 lines

**Notebooks** (1):
- `notebooks/demo.ipynb` - 18 KB, Colab-ready

### 7.2 Quality Assessment

**Strengths**:
- Real-world case studies (Treasury rates)
- Graceful degradation (FRED -> synthetic)
- Complete workflows

**Gaps**:
| Gap | Impact | Recommendation |
|-----|--------|----------------|
| No "hello world" example | High - barrier to entry | Add `00_quickstart.py` (50 lines max) |
| No model comparison example | Medium | Show DM test results table |
| No benchmark usage example | Medium | Add `06_using_benchmarks.py` |
| Demo notebook too brief | Low | Expand to 3x current length |

### 7.3 Missing Example Types

1. **Hyperparameter tuning with nested CV**
2. **Multi-horizon forecast comparison**
3. **Production pipeline integration**
4. **Handling missing values**
5. **Multi-series validation**

---

## 8. Publication Readiness Checklist

### 8.1 Blocking Issues (Must Fix)

| Issue | Location | Action Required |
|-------|----------|-----------------|
| n_shuffles=5 power problem | `SPECIFICATION.md:34` | Increase default or document limitation |
| Benchmark runner missing | `benchmarks/` | Implement `run_benchmarks()` |
| MC-SS formula incomplete | `mathematical_foundations.md` | Add full derivation |
| 3-class PT decision rule | `SPECIFICATION.md:147` | Add usage guidance |

### 8.2 Important Issues (Should Fix)

| Issue | Location | Action Required |
|-------|----------|-----------------|
| No comparison vs sklearn/sktime | README.md | Add benchmark table |
| Missing BibTeX citation | README.md | Add citation section |
| DM test limitations undocumented | `statistical_tests.py` docstring | Add "Limitations" section |
| Conformal exchangeability warning | `conformal.py` | Add diagnostic check |

### 8.3 Nice-to-Have

| Issue | Location | Action Required |
|-------|----------|-----------------|
| Type hint coverage 65% -> 85% | Multiple files | Add parameter hints |
| Quickstart example | `examples/00_quickstart.py` | Create 50-line intro |
| Performance profiling results | README.md | Add timing comparisons |
| Windows path testing | CI/docs | Verify and document |

---

## 9. Improvement Roadmap

### Phase 1: Pre-Release Critical (1-2 days)

1. **Fix n_shuffles power issue**
   - Option A: Increase default to 99
   - Option B: Use analytical permutation p-value
   - Document power analysis

2. **Add MC-SS formal definition** to mathematical_foundations.md

3. **Add 3-class PT decision rule** to SPECIFICATION.md

4. **Create benchmark runner skeleton**

### Phase 2: Documentation Enhancement (2-3 days)

1. Add BibTeX citation to README
2. Create `00_quickstart.py` example
3. Document DM test limitations
4. Add conformal exchangeability guidance

### Phase 3: Benchmarking Infrastructure (3-5 days)

1. Implement `run_benchmarks()` function
2. Add comparison vs sklearn `TimeSeriesSplit`
3. Create reproducible local dataset
4. Publish benchmark results table

### Phase 4: Statistical Extensions (Future)

1. Implement Giacomini-White test
2. Add nested CV support
3. Implement self-normalized DM test
4. Add blocked bootstrap CI for gates

---

## 10. Summary of Recommendations

### Top 5 Critical Actions

1. **Increase `n_shuffles` default** (5 -> 99) or implement analytical p-value
2. **Create benchmark runner** with comparison table
3. **Complete MC-SS mathematical derivation**
4. **Add DM test limitations** to docstring (negative variance, power issues)
5. **Add conformal exchangeability diagnostic**

### Verdict

**temporalcv is fundamentally sound** with exceptional test coverage (9.6/10) and clean architecture. The methodological gaps identified are **surmountable** with focused effort.

**Recommended path to v1.0**:
1. Fix n_shuffles power issue (critical)
2. Add benchmark comparison table (credibility)
3. Document known limitations (transparency)
4. Complete mathematical derivations (rigor)

**Estimated effort**: 3-5 days for blocking issues, 1-2 weeks for full roadmap.

---

## References

1. [Time Series Cross-Validation Best Practices](https://otexts.com/fpp3/tscv.html) - Hyndman & Athanasopoulos
2. [Comparing Predictive Accuracy, Twenty Years Later](https://www.nber.org/system/files/working_papers/w18391/w18391.pdf) - Diebold (2015)
3. [Forecast Evaluation Tests and Negative LRV](https://www.sciencedirect.com/science/article/abs/pii/S0169207017300559) - Coroneo & Iacone (2016)
4. [A Gentle Introduction to Conformal Time Series Forecasting](https://arxiv.org/html/2511.13608) - arXiv 2024
5. [Bellman Conformal Inference](https://arxiv.org/abs/2402.05203) - Yang, Candes & Lei (2024)
6. [Permutation P-values Should Never Be Zero](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2687965/) - Phipson & Smyth (2010)
7. [New Techniques for Time Series CV](https://link.springer.com/article/10.1007/s43069-024-00334-8) - Operations Research Forum (2024)
8. [Self-Normalized DM Test](https://www.arxiv.org/abs/2510.04318) - arXiv 2025
9. [How to Evaluate Python Package Quality](https://realpython.com/python-package-quality/) - Real Python

---

## 11. Hub Integration Compliance (lever_of_archimedes)

**Overall Hub Integration Score**: 75/100

### 11.1 Patterns Adopted

| Pattern | Status | Gap |
|---------|--------|-----|
| 6-Layer Validation (`patterns/testing.md`) | 85% | Missing Layer 6 (Hypothesis property tests) |
| Data Leakage Prevention (`patterns/data_leakage_prevention.md`) | 95% | Excellent - all 10 bug categories mapped |
| DS/ML Lifecycle (`patterns/ds_ml_lifecycle.md`) | 60% | Missing .tracking/ infrastructure |
| Git Commits (`patterns/git.md`) | 90% | Missing robot attribution line |
| Precision Feedback (`patterns/precision.md`) | 0% | Not implemented |
| Session Workflow (`patterns/sessions.md`) | 10% | Only CURRENT_WORK.md exists |

### 11.2 CRITICAL: Missing Infrastructure

| Missing | Impact | Priority | Fix Time |
|---------|--------|----------|----------|
| **`.tracking/` directory** | Lost decision history | CRITICAL | 15 min |
| **`requirements.lock`** | Non-reproducible deployment | CRITICAL | 5 min |
| **`ROADMAP.md`** | Can't track future work | HIGH | 20 min |
| **`SESSION_*.md` files** | Lost context switching | HIGH | 30 min |
| **Model cards for gates** | Users don't understand artifacts | MEDIUM | 1 hour |

### 11.3 Spec/Code Drift Found

| Document | Says | Code Does | Action |
|----------|------|-----------|--------|
| SPECIFICATION.md:208-215 | Conformal min n=50 | Code allows n>=10 | Reconcile |
| docs/knowledge/assumptions.md:55-59 | n>=30 recommended | Code allows less | Document |
| SPECIFICATION.md:34 | n_shuffles=5 | Works but underpowered | Increase to 99 |

### 11.4 Pattern Compliance Actions

**CRITICAL (Before v1.0)**:
1. Generate `requirements.lock`:
   ```bash
   pip-compile pyproject.toml -o requirements.lock
   ```

2. Create `.tracking/` directory:
   ```
   .tracking/
   ├── decisions.md          # Architectural decisions log
   ├── technical_debt.yaml   # Known issues tracking
   └── phase_transitions.log # v0.3->v0.4->v1.0 record
   ```

3. Add robot attribution to commit template

**HIGH (v1.0 release)**:
4. Create `ROADMAP.md` with sprint/backlog structure
5. Add Hypothesis property tests for gates
6. Add model cards for WalkForwardCV, gate_shuffled_target

---

## 12. Cross-Project Comparison

### 12.1 Comparison Matrix

| Aspect | temporalcv | myga-v4 | annuity_forecasting | research-kb |
|--------|-----------|---------|---------------------|-------------|
| **Test Coverage** | 80%+ | 90%+ | 90%+ | Unit + integration |
| **Anti-Pattern Tests** | Organized | Separate category | Separate module | N/A |
| **Leakage Audit Trail** | Comprehensive | Brief | Detailed | N/A |
| **Research KB Integration** | No | Yes (hooks) | Yes | Core product |
| **Pre-commit Hooks** | GitHub Actions | Local + Actions | No | No |
| **.tracking/ Directory** | No | Partial | Yes | N/A |
| **Amendment Process** | Defined | Implicit | Defined | N/A |
| **Phase Tracking** | v1.0-rc1 | Deployment | Phase 7 | Core product |

### 12.2 Patterns to Adopt from Sister Projects

**From myga-forecasting-v4:**
- "Bug-Driven Testing" pattern with explicit `@pytest.mark.anti_pattern` markers
- Suspicious Results Diagnosis Flowchart in docs
- Parametrized model testing across CV strategies

**From annuity_forecasting:**
- Tiered function design (Tier 1: 10-30 lines, Tier 2: 30-50 lines, Tier 3: 50+ with justification)
- "CRITICAL ERROR" prefix format for error messages
- Explicit known limitations section in CLAUDE.md

**From research-kb:**
- Knowledge base integration hooks for semantic search
- Master bibliography with deduplication
- Textbook chapter cross-references

### 12.3 What temporalcv Contributes Back to Hub

| Contribution | Value |
|--------------|-------|
| Episode-Based Postmortem Pattern | Reusable for ML bug documentation |
| Gate Framework (HALT/WARN/PASS/SKIP) | Cleaner than boolean pass/fail |
| Knowledge Tier Exemplar | Reference [T1/T2/T3] implementation |
| Walk-Forward CV with Gap Enforcement | Fills explicit ecosystem gap |

---

## 13. Research & Literature Gaps

### 13.1 Methods NOT in temporalcv (Could Add)

| Method | Purpose | Priority | Reference |
|--------|---------|----------|-----------|
| **Giacomini-White Test** | Conditional predictive ability | Medium | Giacomini & White (2006) |
| **Clark-West Test** | Nested model comparison | Medium | Clark & West (2007) |
| **Reality Check / SPA Test** | Multiple model comparison | Low | Hansen (2005) |
| **Forecast Encompassing** | Model combination | Low | Harvey et al. (1998) |
| **Self-Normalized DM** | Robust to bandwidth selection | High | [arXiv 2025](https://arxiv.org/abs/2510.04318) |
| **Bellman Conformal Inference** | Optimal threshold adaptation | High | [Yang, Candes & Lei 2024](https://arxiv.org/abs/2402.05203) |

### 13.2 Recent Literature (2024-2025)

| Paper | Key Finding | Applicability |
|-------|-------------|---------------|
| [Weighted K-Fold Time Series CV](https://link.springer.com/article/10.1007/s43069-024-00334-8) | Exponential weighting beats uniform | Could add to WalkForwardCV |
| [Attention-Based Conformal Prediction](https://arxiv.org/html/2511.15838) | Feature-space nonconformity scores | Extension to AdaptiveConformalPredictor |
| [Neural Conformal Control](https://arxiv.org/html/2412.18144) | End-to-end learning for conformal | Future v2.0 direction |
| [Diebold 2015 Retrospective](https://www.nber.org/papers/w18391) | DM for forecasts, NOT models | Add warning to docstring |

---

## 14. Coding Standards Alignment

### 14.1 Current Alignment with Hub Standards

| Standard | Hub | temporalcv | Status |
|----------|-----|-----------|--------|
| Line length | 100 chars | 100 chars | Aligned |
| Formatter | Black + Ruff | Ruff | Aligned |
| Type checker | mypy strict | mypy strict | Aligned |
| Docstrings | NumPy style | NumPy style | Aligned |
| Coverage target | 80%+ modules, 90%+ core | 80%+ | Could raise core to 90% |
| Function length | 20-50 lines | Not enforced | Add tiered approach |
| Error prefix | "CRITICAL ERROR" | Custom | Adopt hub pattern |

### 14.2 Pre-Commit Configuration Gap

**Current**: GitHub Actions only (no local hooks)

**Recommended `.pre-commit-config.yaml`**:
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.8.0
    hooks:
      - id: ruff
        args: ['--select=E,W,F,I', '--line-length=100']
      - id: ruff-format
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.13.0
    hooks:
      - id: mypy
        additional_dependencies: ['types-all']
```

---

## 15. Extended Recommendations

### 15.1 Immediate Actions (Before v1.0)

| # | Action | Time | Impact |
|---|--------|------|--------|
| 1 | Generate `requirements.lock` | 5 min | CRITICAL |
| 2 | Create `.tracking/` directory | 15 min | CRITICAL |
| 3 | Fix n_shuffles power (99 or analytical) | 30 min | CRITICAL |
| 4 | Add ROADMAP.md | 20 min | HIGH |
| 5 | Reconcile spec/code drift (conformal n) | 15 min | HIGH |
| 6 | Add RELEASE_NOTES.md | 30 min | HIGH |

### 15.2 High Priority (v1.0 Release)

| # | Action | Time | Impact |
|---|--------|------|--------|
| 7 | Add Hypothesis property tests | 2-3 hrs | HIGH |
| 8 | Create benchmark comparison table | 2 hrs | HIGH |
| 9 | Add DM test limitations to docstring | 30 min | MEDIUM |
| 10 | Add conformal exchangeability diagnostic | 1 hr | MEDIUM |
| 11 | Add robot attribution to commits | 10 min | LOW |

### 15.3 Medium Priority (v1.1)

| # | Action | Time | Impact |
|---|--------|------|--------|
| 12 | Implement self-normalized DM test | 4 hrs | MEDIUM |
| 13 | Add model cards for gates | 2 hrs | MEDIUM |
| 14 | Implement nested CV | 4 hrs | MEDIUM |
| 15 | Add research-kb integration hooks | 2 hrs | LOW |
| 16 | Expand leakage episodes (4, 5) | 2 hrs | LOW |

### 15.4 Path to 5-Star Hub Integration

Current: 4 stars (75/100)

**To reach 100/100:**
1. Complete .tracking/ infrastructure (critical)
2. Add requirements.lock (critical)
3. Add Hypothesis property tests (Layer 6)
4. Implement session workflow
5. Add precision feedback to CLAUDE.md

**Estimated time to 5-star**: 4-6 hours focused work

---

## Appendix A: File References

| Category | Key Files |
|----------|-----------|
| Core Implementation | `cv.py:1223`, `gates.py:1541`, `statistical_tests.py:891` |
| Tests | `tests/test_gates.py:698`, `tests/anti_patterns/test_lag_leakage.py:221` |
| Documentation | `SPECIFICATION.md:299`, `docs/knowledge/mathematical_foundations.md:348` |
| Benchmarks | `src/temporalcv/benchmarks/base.py`, `tests/benchmarks/test_cv_benchmarks.py:281` |
| Examples | `examples/01_leakage_detection.py:387` |
| Hub Patterns | `~/Claude/lever_of_archimedes/patterns/` |
| Sister Projects | `~/Claude/myga-forecasting-v4/`, `~/Claude/annuity_forecasting/` |

---

## Appendix B: Burst Implementation Sessions

### Session 1: Release Prep (25-min burst)
```
/burst 25
- Generate requirements.lock
- Create .tracking/ directory structure
- Create RELEASE_NOTES.md
Commit: "release: v1.0.0 preparation (requirements.lock, tracking, notes)"
```

### Session 2: Power Fix (25-min burst)
```
/burst 25
- Update n_shuffles default to 99 in SPECIFICATION.md
- Update code in gates.py
- Add power analysis note to docs
Commit: "fix: Increase n_shuffles default for adequate power"
```

### Session 3: Hub Compliance (25-min burst)
```
/burst 25
- Add ROADMAP.md
- Update CLAUDE.md with precision feedback section
- Add robot attribution to commit template
Commit: "docs: Hub compliance improvements (roadmap, attribution)"
```

### Session 4: Property Tests (Multi-burst)
```
/burst 25 - Create tests/property/test_gates_invariants.py
/burst 25 - Complete gate invariants (3-4 tests)
/burst 25 - Add CV and metric property tests
Commit: "test: Add Hypothesis property tests (Layer 6)"
```

---

## Appendix C: Verdict Summary

| Dimension | Score | Assessment |
|-----------|-------|------------|
| Code Quality | 9.2/10 | Excellent - publication-ready |
| Test Coverage | 9.6/10 | Exceptional - 1,338+ tests |
| Documentation | 8.5/10 | Strong - minor gaps |
| ML Methodology | 7.5/10 | **Needs attention** - n_shuffles power, DM limitations |
| Benchmarking | 6.5/10 | **Significant gaps** - no comparison table |
| Statistical Rigor | 8.8/10 | Strong - verified implementations |
| Hub Integration | 7.5/10 | **Missing infrastructure** - .tracking/, requirements.lock |

**Overall**: temporalcv is **fundamentally excellent** with exceptional test coverage and rigorous statistical foundations. The gaps are primarily in:
1. Infrastructure compliance with hub patterns
2. Power analysis for permutation tests
3. Benchmark comparison evidence
4. Documentation of known limitations

**Recommended investment**: 1-2 days for blocking issues, 3-5 days for full 5-star hub integration.

---

*Generated 2025-12-24 by Claude Opus 4.5*
